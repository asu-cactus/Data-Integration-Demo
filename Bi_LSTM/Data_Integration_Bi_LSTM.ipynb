{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Integration Bi-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uLLB3gl6AvL"
      },
      "source": [
        "## Install Deps\n",
        "- Tensorflow\n",
        "- Sklearn\n",
        "- NLTK\n",
        "- fastText\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF9Mm8JEb9YD"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15\n",
        "!pip install nltk\n",
        "!pip install fasttext\n",
        "!pip install tflearn\n",
        "!pip install sklearn\n",
        "!pip install fasttext\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_e7FhTB6V6V"
      },
      "source": [
        "import tensorflow as tf\n",
        "import fasttext\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import heapq\n",
        "import time\n",
        "import fasttext\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from tflearn.data_utils import pad_sequences\n",
        "from sklearn.metrics import average_precision_score,accuracy_score,precision_score, recall_score, f1_score\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.contrib import rnn\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqGIcqCDeCav"
      },
      "source": [
        "## Download and prepare the dataset\n",
        "- Data_Integration_Dataset\n",
        " - Covid-*19*\n",
        " - Machine_Log"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LTCz3e8d_Y9"
      },
      "source": [
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=19oLAKktjI0uk8v4lcdBTnRBTyqN-tGeR', output=None, quiet=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV0Yxs_TeAU-"
      },
      "source": [
        "!unzip -qq Data_Integration_Dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfrM89bo7iMY"
      },
      "source": [
        "## Training word embedding using fastText\n",
        "- training algorithm: skipgram\n",
        "- training epoches: 10\n",
        "- embedding dimension: 150\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKOk17U97fyU"
      },
      "source": [
        "model = fasttext.train_unsupervised('/content/data_corpus_path.txt', 'skipgram',\n",
        "                                    epoch=10, minn=2, maxn=5, dim=150, thread=16)\n",
        "model.save_model('/content/word_embedding_model.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mal1PeRP8GTV"
      },
      "source": [
        "## Model: Bi-LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn1YbHmVb-kY"
      },
      "source": [
        "class bi_lstm(object):\n",
        "    def __init__(self, max_document_length, num_class,vocab_size,embedding_size,trainset_embedding=None):\n",
        "        self.lr = 0.001\n",
        "        self.num_hidden = 512\n",
        "        self.fc_num_hidden = 256\n",
        "\n",
        "        self.x = tf.placeholder(tf.int32, [None, max_document_length],name = 'input_x')\n",
        "        self.x_len = tf.reduce_sum(tf.sign(self.x), 1)\n",
        "        self.y = tf.placeholder(tf.float32, [None,num_class], name = 'input_y')\n",
        "        self.keep_prob = tf.placeholder(tf.float32, [],name = 'dropout')\n",
        "        self.x_init = tf.placeholder(tf.float32, shape=(vocab_size,embedding_size),name = 'x_init')\n",
        "        self.global_step = tf.Variable(0, trainable=False, name=\"Global_Step\")\n",
        "\n",
        "\n",
        "\n",
        "        with  tf.device(\"/cpu:0\"),tf.variable_scope(\"embedding\"):\n",
        "\n",
        "            embeddings = tf.Variable(self.x_init, dtype=tf.float32,trainable=True,name=\"pretrained_embedding\")\n",
        "            train_embeddings = tf.Variable(trainset_embedding, dtype=tf.float32,\n",
        "                                           trainable=True,name=\"embs_only_in_train\")\n",
        "\n",
        "            embeddings = tf.concat([embeddings, train_embeddings], axis=0)\n",
        "            x_emb = tf.nn.embedding_lookup(embeddings, self.x)\n",
        "     \n",
        "\n",
        "        with tf.variable_scope(\"rnn\"):\n",
        "            lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(self.num_hidden)  # forward direction cell\n",
        "            lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(self.num_hidden)  # backward direction cell  \n",
        "\n",
        "            rnn_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
        "                lstm_fw_cell,lstm_bw_cell, x_emb, sequence_length=self.x_len, dtype=tf.float32)\n",
        "         \n",
        "            # Concat output\n",
        "            lstm_concat = tf.concat(rnn_outputs, axis=2)  # [batch_size, sequence_length, lstm_hidden_size * 2]\n",
        "            lstm_out = tf.reduce_mean(lstm_concat, axis=1)  # [batch_size, lstm_hidden_size * 2]\n",
        "\n",
        "\n",
        "\n",
        "        with tf.name_scope(\"fc\"):\n",
        "            fc_output = tf.layers.dense(lstm_out, self.fc_num_hidden, activation=tf.nn.relu)\n",
        "            dropout = tf.nn.dropout(fc_output, self.keep_prob)\n",
        "\n",
        "  \n",
        "        with tf.name_scope(\"output\"):\n",
        "            W = tf.Variable(tf.truncated_normal(shape=[self.fc_num_hidden, num_class],\n",
        "                                                stddev=0.1, dtype=tf.float32), name=\"W\")\n",
        "            b = tf.Variable(tf.constant(value=0.1, shape=[num_class], dtype=tf.float32), name=\"b\")\n",
        "            self.logits = tf.nn.xw_plus_b(dropout, W, b, name=\"logits\")\n",
        "            self.scores = tf.sigmoid(self.logits, name=\"scores\")\n",
        "           \n",
        "\n",
        "        with tf.name_scope(\"loss\"):\n",
        "\n",
        "            self.loss = tf.reduce_mean(tf.reduce_sum(\n",
        "                tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.y),axis = 1))\n",
        "            self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.loss,\n",
        "                                                                                 global_step=self.global_step)\n",
        "\n",
        "        with tf.name_scope(\"lr\"):\n",
        "            self.lr = tf.Variable(0.001, name='learning_rate', trainable=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ySsznG69VOo"
      },
      "source": [
        "# Define Util Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lISVY2p7cAXy"
      },
      "source": [
        "def clean_str(text):\n",
        "    '''\n",
        "    regular expression to clean text file\n",
        "\n",
        "    '''\n",
        "\n",
        "    text = re.sub(r\"[_{},-/:.!?%â€™\\'\\\"]\", \" \", str(text))\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", str(text))\n",
        "    text = text.strip().lower()\n",
        "    text = text.lower()\n",
        "    text = text.replace('[', '')\n",
        "    text = text.replace(']', '')\n",
        "\n",
        "    return text\n",
        "\n",
        "def create_onehot_labels(labels_index,num_labels):\n",
        "    '''\n",
        "    create onehot label vector\n",
        "\n",
        "    :param labels_index: preset order of label\n",
        "    :param num_labels: number of classes\n",
        "    :return: onehot label vector\n",
        "    '''\n",
        "    label = [0] * num_labels\n",
        "    for item in labels_index:\n",
        "\n",
        "        label[int(item-1)] = 1\n",
        "\n",
        "\n",
        "    return label\n",
        "\n",
        "\n",
        "def cos_sim(vector_a, vector_b):\n",
        "    \"\"\"\n",
        "    calculate the cosine similarity between two vectors\n",
        "\n",
        "    :param vector_a: vector a\n",
        "    :param vector_b: vector b\n",
        "    :return: cosine similary\n",
        "    \"\"\"\n",
        "\n",
        "    vector_a = np.mat(vector_a)\n",
        "    vector_b = np.mat(vector_b)\n",
        "    num = float(vector_a * vector_b.T)\n",
        "    denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
        "    sim = num / denom\n",
        "    return sim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_data_word2vec(TRAIN_PATH,num_class,vocab_size, embed_size, embedding_model):\n",
        "    '''\n",
        "    create the training set(train_x) and labels(train_y)\n",
        "\n",
        "    :param TRAIN_PATH: training data file\n",
        "    :param num_class: total number of classes(rows + attribute)\n",
        "    :param vocab_size: number of total vacabulary in pretrained embedding\n",
        "    :param embed_size: embedding size for each word(word vector dimension)\n",
        "    :param embedding_model: pretrained embedding model file\n",
        "    :return: content_index_list, word vector index in embedding matrix\n",
        "             onehot_labels_list, word label vector\n",
        "             trainset_embedding_matrix, oov word(from training dataset) embedding matrix\n",
        "             oov_word, oov word in training dataset\n",
        "    '''\n",
        "\n",
        "    model = fasttext.load_model(embedding_model)\n",
        "    vocab = dict([(word, model.get_word_id(word)) for word in model.get_words()])\n",
        "\n",
        "    content_index_list = []\n",
        "    onehot_labels_list = []\n",
        "    label_num_list = []\n",
        "    trainset_embedding_matrix = np.zeros((0, embed_size))\n",
        "    oov_word = []\n",
        "\n",
        "    count = 0\n",
        "\n",
        "  \n",
        "    with open(os.path.join(folder_path,TRAIN_PATH), encoding='utf-8') as f:\n",
        "        for line in f.readlines():\n",
        "            df = json.loads(line)\n",
        "            content = df['value']\n",
        "\n",
        "            result = []\n",
        "\n",
        "            for item in word_tokenize(clean_str(content)):\n",
        "                word2id = vocab.get(item)\n",
        "                if word2id is None and item not in oov_word:\n",
        "                    oov_word.append(item)\n",
        "\n",
        "                    word_vec = model.get_word_vector(item)\n",
        "                    trainset_embedding_matrix = np.insert(trainset_embedding_matrix,\n",
        "                                                          len(trainset_embedding_matrix), values=word_vec, axis=0)\n",
        "                    word2id = len(model.get_words()) + count\n",
        "                    count += 1\n",
        "\n",
        "                elif word2id is None and item in oov_word:\n",
        "                    word2id = vocab_size + oov_word.index(item)\n",
        "\n",
        "                result.append(word2id)\n",
        "            content_index_list.append(result)\n",
        "\n",
        "            label_list = df[\"label_index\"]\n",
        "            num_label = df['label_number']\n",
        "\n",
        "            onehot_labels_list.append(create_onehot_labels(label_list, num_class))\n",
        "            label_num_list.append(num_label)\n",
        "    \n",
        "   \n",
        "    return content_index_list, onehot_labels_list, label_num_list,trainset_embedding_matrix,oov_word\n",
        "\n",
        "\n",
        "def test_data_word2vec(TEST_PATH,num_class,vocab_size,embedding_model,oov_word):\n",
        "    '''\n",
        "    create the testing set(test_x) and labels(test_y)\n",
        "\n",
        "    :param TEST_PATH: test dataset\n",
        "    :param num_class: total number of classes(rows + attribute)\n",
        "    :param vocab_size: number of total vocabulary in pretrained embedding\n",
        "    :param embedding_model: pretrained embedding model file\n",
        "    :param oov_word: oov word in training dataset\n",
        "\n",
        "    :return: content_index_list, word vector index in embedding matrix\n",
        "             onehot_labels_list, word label vector\n",
        "\n",
        "    '''\n",
        "\n",
        "\n",
        "    model = fasttext.load_model(embedding_model)\n",
        "    vocab = dict([(word, model.get_word_id(word)) for word in model.get_words()])\n",
        "\n",
        "    oov_vocab = dict([(word, oov_word.index(word)+vocab_size) for word in oov_word])\n",
        "    whole_vocab ={}\n",
        "    whole_vocab.update(vocab)\n",
        "    whole_vocab.update(oov_vocab)\n",
        "\n",
        "\n",
        "\n",
        "    content_index_list = []\n",
        "    onehot_labels_list = []\n",
        "    label_number_list = []\n",
        "    oov_list = []\n",
        "\n",
        "    with open(os.path.join(folder_path,TEST_PATH), encoding='utf-8') as f:\n",
        "        for line in f.readlines():\n",
        "\n",
        "            df = json.loads(line)\n",
        "            content = df['value']\n",
        "\n",
        "            result = []\n",
        "\n",
        "            for item in word_tokenize(clean_str(content)):\n",
        "                word2id = whole_vocab.get(item)\n",
        "                if word2id is None:\n",
        "\n",
        "                    word2id = 0\n",
        "                    oov_list.append(item)\n",
        "                result.append(word2id)\n",
        "            content_index_list.append(result)\n",
        "\n",
        "\n",
        "            label_list = df[\"label_index\"]\n",
        "            num_label = df['label_number']\n",
        "            onehot_labels_list.append(create_onehot_labels(label_list, num_class))\n",
        "            label_number_list.append(num_label)\n",
        "\n",
        "\n",
        "    return content_index_list, onehot_labels_list,label_number_list\n",
        "\n",
        "\n",
        "def load_word2vec_matrix(embedding_model):\n",
        "    '''\n",
        "    create pretrained word embedding matrix\n",
        "\n",
        "    :param embedding_model: pretrained embedding model file\n",
        "\n",
        "    :return:\n",
        "             vocab_size, number of total vacabulary in pretrained embedding\n",
        "             embedding_size, embedding size for each word(word vector dimension)\n",
        "             embedding_matrix, word embedding matrix\n",
        "    '''\n",
        "\n",
        "\n",
        "    model = fasttext.load_model(embedding_model)\n",
        "    vocab_size = (model.get_output_matrix()).shape[0]\n",
        "    embedding_size = model.get_dimension()\n",
        "\n",
        "    vocab = dict([(word, model.get_word_id(word)) for word in model.get_words()])\n",
        "\n",
        "    embedding_matrix = np.zeros([vocab_size, embedding_size])\n",
        "    for word, index in vocab.items():\n",
        "        if word is not None:\n",
        "            embedding_matrix[index] = model[word]\n",
        "    \n",
        "    return vocab_size, embedding_size, embedding_matrix\n",
        "\n",
        "\n",
        "\n",
        "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
        "    '''\n",
        "\n",
        "    :param inputs: unbatched data\n",
        "    :param outputs: batched data\n",
        "    :param batch_size: size of every data batch\n",
        "    :param num_epochs: number of epochs\n",
        "\n",
        "    :return:\n",
        "           A batch iterator for data set\n",
        "    '''\n",
        "    inputs = np.array(inputs)\n",
        "    outputs = np.array(outputs)\n",
        "\n",
        "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
        "            yield inputs[start_index:end_index], outputs[start_index:end_index]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwjBTyFTcCza"
      },
      "source": [
        "\n",
        "def get_onehot_label_topk(scores, top_num):\n",
        "    '''\n",
        "\n",
        "    get the top k score from testing result,\n",
        "   \n",
        "\n",
        "    :param scores:  predicted scores for each classification class\n",
        "    :param top_num: number of labels for each data(corrosponding to top k scores)\n",
        "   \n",
        "\n",
        "    :return:\n",
        "        predicted_onehot_labels: Predict labels (onehot format)\n",
        "    '''\n",
        "\n",
        "    predicted_onehot_labels = []\n",
        "    score = np.ndarray.tolist(scores)\n",
        "    # for score in scores:\n",
        "    \n",
        "    onehot_labels_list = [0] * len(score)\n",
        "    max_num_index_list = list(map(score.index, heapq.nlargest(top_num, score)))\n",
        "   \n",
        "    \n",
        "    for i in max_num_index_list:\n",
        "        onehot_labels_list[i] = 1\n",
        "    predicted_onehot_labels.append(onehot_labels_list)\n",
        "\n",
        "    return predicted_onehot_labels\n",
        "\n",
        "def get_onehot_label_threshold(scores, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Get the predicted one-hot labels based on the threshold.\n",
        "    If there is no predict score greater than threshold, then choose the label which has the max predict score.\n",
        "    Args:\n",
        "        scores: The all classes predicted scores provided by network.\n",
        "        threshold: The threshold (default: 0.5).\n",
        "    Returns:\n",
        "        predicted_onehot_labels: The predicted labels (one-hot).\n",
        "    \"\"\"\n",
        "    predicted_onehot_labels = []\n",
        "    score = np.ndarray.tolist(scores)\n",
        "    # for score in scores:\n",
        "    count = 0\n",
        "    onehot_labels_list = [0] * len(score)\n",
        "    for index, predict_score in enumerate(score):\n",
        "        if predict_score >= threshold:\n",
        "            onehot_labels_list[index] = 1\n",
        "            count += 1\n",
        "    if count == 0:\n",
        "        max_score_index = score.index(max(score))\n",
        "        onehot_labels_list[max_score_index] = 1\n",
        "    predicted_onehot_labels.append(onehot_labels_list)\n",
        "    return predicted_onehot_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSd6YBSv9O9n"
      },
      "source": [
        "# Define Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSxd5e-xcEOG"
      },
      "source": [
        "\n",
        "def train(train_x, train_y, test_x, test_y,vocab_size,\n",
        "          embedding_size, pretrained_embedding, trainset_embedding_matrix):\n",
        "    '''\n",
        "    traing process + testing process\n",
        "\n",
        "    :param train_x: training dataset\n",
        "    :param train_y: training label\n",
        "    :param test_x: testing dataset\n",
        "    :param test_y: testing label\n",
        "    :param vocab_size: number of vocabulary in embedding matrx\n",
        "    :param embedding_size: embedding size for each word\n",
        "    :param pretrained_embedding: pretrained word embedding matrix\n",
        "    :param trainset_embedding_matrix: oov word(from train set) embedding matrix\n",
        "    :param args:\n",
        "\n",
        "    :return:\n",
        "            print testing result by fixed epoch interval\n",
        "    '''\n",
        "\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        model = bi_lstm(MAX_DOCUMENT_LEN, NUM_CLASS,vocab_size = vocab_size,embedding_size= embedding_size,\n",
        "                         trainset_embedding=trainset_embedding_matrix)\n",
        "\n",
        "\n",
        "        # Define training procedure\n",
        "        global_step = tf.Variable(0, trainable=False)\n",
        "        params = tf.trainable_variables()\n",
        "        gradients = tf.gradients(model.loss, params)\n",
        "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "        optimizer = tf.train.AdamOptimizer(model.lr)\n",
        "        train_op = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=global_step)\n",
        "\n",
        "        # Initialize all variables\n",
        "        feed_dict_emb = {\n",
        "            model.x_init: np.float32(pretrained_embedding)\n",
        "        }\n",
        "        sess.run(tf.global_variables_initializer(),feed_dict=feed_dict_emb)\n",
        "\n",
        "\n",
        "        def train_step(batch_x, batch_y):\n",
        "            feed_dict = {\n",
        "                model.x: batch_x,\n",
        "                model.y: batch_y,\n",
        "                model.keep_prob: 0.8,\n",
        "\n",
        "            }\n",
        "            _, step, loss = sess.run([train_op, global_step, model.loss], feed_dict=feed_dict)\n",
        "\n",
        "            return loss\n",
        "\n",
        "        def test_accuracy(test_x, test_y):\n",
        "            '''\n",
        "\n",
        "            :param test_x: testing dataset\n",
        "            :param test_y: testing label\n",
        "            :return:\n",
        "                eval_loss: loss\n",
        "                accuracy: accuracy\n",
        "                ave_precison_score: average precison\n",
        "\n",
        "            '''\n",
        "\n",
        "\n",
        "            true_onehot_labels = []\n",
        "            predicted_onehot_scores = []\n",
        "\n",
        "\n",
        "            predicted_onehot_labels_t2 = []\n",
        "\n",
        "            test_batches = batch_iter(test_x, test_y, BATCH_SIZE, 1)\n",
        "            eval_loss, eval_counter = 0., 0\n",
        "\n",
        "\n",
        "            for test_batch_x, test_batch_y in test_batches:\n",
        "                scores, cur_loss = sess.run([model.scores, model.loss],\n",
        "                                            feed_dict={model.x: test_batch_x, model.y: test_batch_y,\n",
        "                                                       model.keep_prob: 1.0})\n",
        "\n",
        "                for labels,score in zip(test_batch_y,scores):\n",
        "                    true_onehot_labels.append(labels)\n",
        "                    predicted_onehot_scores.append(score)\n",
        "                   \n",
        "                    number_label = list(labels).count(1)\n",
        "                    \n",
        "                    batch_predicted_onehot_labels = get_onehot_label_topk(scores=score, top_num=number_label)\n",
        "                   \n",
        "                    predicted_onehot_labels_t2.append(batch_predicted_onehot_labels[0])\n",
        "                   \n",
        "                eval_loss = eval_loss + cur_loss\n",
        "                eval_counter = eval_counter + 1\n",
        "\n",
        "            #metrics\n",
        "            eval_loss = float(eval_loss / eval_counter)\n",
        "\n",
        "            accuracy = accuracy_score(np.array(true_onehot_labels), np.array(predicted_onehot_labels_t2))\n",
        "            avg_precision = average_precision_score(np.array(true_onehot_labels), np.array(predicted_onehot_labels_t2),average = 'micro')\n",
        "            precision = precision_score(np.array(true_onehot_labels), np.array(predicted_onehot_labels_t2),average = 'micro')\n",
        "            recall = recall_score(np.array(true_onehot_labels), np.array(predicted_onehot_labels_t2),average = 'micro')\n",
        "         \n",
        "            f1 = f1_score(np.array(true_onehot_labels), np.array(predicted_onehot_labels_t2),average='micro')\n",
        "           \n",
        "            \n",
        "            return eval_loss,avg_precision, accuracy,precision,recall,f1\n",
        "\n",
        "\n",
        "        # Training loop\n",
        "       \n",
        "\n",
        "        batches = batch_iter(train_x, train_y, BATCH_SIZE, NUM_EPOCHS)\n",
        "\n",
        "        \n",
        "        steps_per_epoch = int(num_train / BATCH_SIZE)\n",
        "        for batch_x, batch_y in batches:\n",
        "            step = tf.train.global_step(sess, global_step)\n",
        "            num_epoch = int(step / steps_per_epoch)\n",
        "\n",
        "            loss = train_step(batch_x, batch_y)\n",
        "\n",
        "            if step % 100 == 0:\n",
        "            \n",
        "            \n",
        "                eval_loss,avg_precision, acc,precision,recall,f1 = test_accuracy(test_x, test_y)\n",
        "            \n",
        "            \n",
        "                print(\"epoch: {}, step: {}, loss: {}, steps_per_epoch: {}, batch size: {}\".\n",
        "                     format(num_epoch, step, eval_loss, steps_per_epoch, BATCH_SIZE))\n",
        "            \n",
        "                print(\"avg_precision:{}, accuracy:{}, precision: {}, f1: {}, recall: {}\".format(avg_precision,acc,precision,f1,recall))\n",
        "                print(\"time of one epoch: {}\\n\".format(time.time()-st))\n",
        "                \n",
        "\n",
        "        \n",
        "        \n",
        "        eval_loss,avg_precision, acc,precision,recall,f1 = test_accuracy(test_x, test_y)\n",
        "        print(\"avg_precision:{}, accuracy:{}, precision: {}, f1: {}, recall: {}\".format(avg_precision,acc,precision,f1,recall))\n",
        "  \n",
        "        \n",
        "\n",
        "       \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-02GSry8sFC"
      },
      "source": [
        "# Training Process \n",
        "\n",
        "## Hyperparameters\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGELBwGWego4"
      },
      "source": [
        "NUM_CLASS = NUM_LABELS \n",
        "\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 50\n",
        "MAX_DOCUMENT_LEN = 30\n",
        "\n",
        "THRESHOLD = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5SStH5QjuzU"
      },
      "source": [
        "# word embedding model \n",
        "MODEL_PATH = '/content/word_embedding_model.bin'\n",
        "\n",
        "folder_path = '/content/Data_Integration_Dataset/Covid-19/'\n",
        "TRAIN_PATH = \"training_covid-19_data_with_column_label_prediction.json\"\n",
        "TEST_PATH = \"testing_covid-19_data_with_column_label_prediction.json\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwXk1An58ive"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtZyYdD_8haE"
      },
      "source": [
        "\n",
        "with open(os.path.join(folder_path,TRAIN_PATH), encoding='utf-8') as f:\n",
        "  num_Train = len(f.readlines())\n",
        "with open(os.path.join(folder_path,TEST_PATH), encoding='utf-8') as f:\n",
        "  num_Test = len(f.readlines())\n",
        "\n",
        "with open(os.path.join(folder_path,TRAIN_PATH), encoding='utf-8') as f1:\n",
        "  train_data_json = [json.loads(i) for i in f1.readlines()]\n",
        "\n",
        "\n",
        "dats = []\n",
        "for i in train_data_json:\n",
        "  dats.extend(i['label_index'])\n",
        "NUM_LABELS = len(set(dats))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiReJCElcFe-"
      },
      "source": [
        "vocab_size, embedding_size, embedding_matrix = load_word2vec_matrix(MODEL_PATH)\n",
        "train_x_index_list, train_y, train_label_num_list,trainset_embedding_matrix, oov_word = train_data_word2vec(\n",
        "    TRAIN_PATH,NUM_CLASS,vocab_size,embedding_size, MODEL_PATH)\n",
        "train_x = pad_sequences(train_x_index_list, maxlen=MAX_DOCUMENT_LEN, value=0.)\n",
        "\n",
        "\n",
        "test_x_index_list, test_y, test_label_num_list= test_data_word2vec(TEST_PATH,NUM_CLASS, vocab_size,MODEL_PATH, oov_word)\n",
        "test_x = pad_sequences(test_x_index_list, maxlen=MAX_DOCUMENT_LEN, value=0.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyoZziWR8_19"
      },
      "source": [
        "# Model Training and Testing Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "802IPqh1cGqf"
      },
      "source": [
        " %time train(train_x, train_y, test_x, test_y,vocab_size,embedding_size, embedding_matrix,trainset_embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}