{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Integration Bi-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "96tUC8q1byy9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF9Mm8JEb9YD"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn1YbHmVb-kY"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn\n",
        "\n",
        "\n",
        "class WordRNN(object):\n",
        "    def __init__(self, max_document_length, num_class,vocab_size,embedding_size,trainset_embedding=None):\n",
        "        self.lr = 0.001\n",
        "        self.num_hidden = 512\n",
        "        self.fc_num_hidden = 256\n",
        "\n",
        "        self.x = tf.placeholder(tf.int32, [None, max_document_length],name = 'input_x')\n",
        "        self.x_len = tf.reduce_sum(tf.sign(self.x), 1)\n",
        "        self.y = tf.placeholder(tf.float32, [None,num_class], name = 'input_y')\n",
        "        self.keep_prob = tf.placeholder(tf.float32, [],name = 'dropout')\n",
        "        self.x_init = tf.placeholder(tf.float32, shape=(vocab_size,embedding_size),name = 'x_init')\n",
        "        self.global_step = tf.Variable(0, trainable=False, name=\"Global_Step\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        with  tf.device(\"/cpu:0\"),tf.variable_scope(\"embedding\"):\n",
        "\n",
        "            embeddings = tf.Variable(self.x_init, dtype=tf.float32,trainable=True,name=\"pretrained_embedding\")\n",
        "            train_embeddings = tf.Variable(trainset_embedding, dtype=tf.float32,\n",
        "                                           trainable=True,name=\"embs_only_in_train\")\n",
        "\n",
        "            embeddings = tf.concat([embeddings, train_embeddings], axis=0)\n",
        "            x_emb = tf.nn.embedding_lookup(embeddings, self.x)\n",
        "            print('....',x_emb)\n",
        "\n",
        "        with tf.variable_scope(\"rnn\"):\n",
        "            lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(self.num_hidden)  # forward direction cell\n",
        "            lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(self.num_hidden)  # backward direction cell\n",
        "      \n",
        "\n",
        "            rnn_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
        "                lstm_fw_cell,lstm_bw_cell, x_emb, sequence_length=self.x_len, dtype=tf.float32)\n",
        "          \n",
        "\n",
        "            # Concat output\n",
        "            lstm_concat = tf.concat(rnn_outputs, axis=2)  # [batch_size, sequence_length, lstm_hidden_size * 2]\n",
        "            lstm_out = tf.reduce_mean(lstm_concat, axis=1)  # [batch_size, lstm_hidden_size * 2]\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "        with tf.name_scope(\"fc\"):\n",
        "            fc_output = tf.layers.dense(lstm_out, self.fc_num_hidden, activation=tf.nn.relu)\n",
        "            dropout = tf.nn.dropout(fc_output, self.keep_prob)\n",
        "\n",
        " \n",
        "\n",
        "        with tf.name_scope(\"output\"):\n",
        "            W = tf.Variable(tf.truncated_normal(shape=[self.fc_num_hidden, num_class],\n",
        "                                                stddev=0.1, dtype=tf.float32), name=\"W\")\n",
        "            b = tf.Variable(tf.constant(value=0.1, shape=[num_class], dtype=tf.float32), name=\"b\")\n",
        "            self.logits = tf.nn.xw_plus_b(dropout, W, b, name=\"logits\")\n",
        "            self.scores = tf.sigmoid(self.logits, name=\"scores\")\n",
        "           \n",
        "\n",
        "        with tf.name_scope(\"loss\"):\n",
        "\n",
        "            self.loss = tf.reduce_mean(tf.reduce_sum(\n",
        "                tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.y),axis = 1))\n",
        "            self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.loss,\n",
        "                                                                                 global_step=self.global_step)\n",
        "\n",
        "        with tf.name_scope(\"lr\"):\n",
        "            self.lr = tf.Variable(0.001, name='learning_rate', trainable=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeFgY6xAb-Db"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install fasttext\n",
        "!pip install tflearn\n",
        "!pip install sklearn\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lISVY2p7cAXy"
      },
      "source": [
        "import fasttext\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "def clean_str(text):\n",
        "    '''\n",
        "    regular expression to clean text file\n",
        "\n",
        "    '''\n",
        "\n",
        "    text = re.sub(r\"[_{},:.!?%â€™\\'\\\"]\", \" \", str(text))\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", str(text))\n",
        "    text = text.strip().lower()\n",
        "    text = text.lower()\n",
        "    text = text.replace('[', '')\n",
        "    text = text.replace(']', '')\n",
        "\n",
        "    return text\n",
        "\n",
        "def create_onehot_labels(labels_index,num_labels):\n",
        "    '''\n",
        "    create onehot label vector\n",
        "\n",
        "    :param labels_index: preset order of label\n",
        "    :param num_labels: number of classes\n",
        "    :return: onehot label vector\n",
        "    '''\n",
        "    label = [0] * num_labels\n",
        "    for item in labels_index:\n",
        "\n",
        "        label[int(item-1)] = 1\n",
        "\n",
        "\n",
        "    return label\n",
        "\n",
        "\n",
        "def cos_sim(vector_a, vector_b):\n",
        "    \"\"\"\n",
        "    calculate the cosine similarity between two vectors\n",
        "\n",
        "    :param vector_a: vector a\n",
        "    :param vector_b: vector b\n",
        "    :return: cosine similary\n",
        "    \"\"\"\n",
        "\n",
        "    vector_a = np.mat(vector_a)\n",
        "    vector_b = np.mat(vector_b)\n",
        "    num = float(vector_a * vector_b.T)\n",
        "    denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
        "    sim = num / denom\n",
        "    return sim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_data_word2vec(TRAIN_PATH,num_class,vocab_size, embed_size, embedding_model):\n",
        "    '''\n",
        "    create the training set(train_x) and labels(train_y)\n",
        "\n",
        "    :param TRAIN_PATH: training data file\n",
        "    :param num_class: total number of classes(rows + attribute)\n",
        "    :param vocab_size: number of total vacabulary in pretrained embedding\n",
        "    :param embed_size: embedding size for each word(word vector dimension)\n",
        "    :param embedding_model: pretrained embedding model file\n",
        "    :return: content_index_list, word vector index in embedding matrix\n",
        "             onehot_labels_list, word label vector\n",
        "             trainset_embedding_matrix, oov word(from training dataset) embedding matrix\n",
        "             oov_word, oov word in training dataset\n",
        "    '''\n",
        "\n",
        "    model = fasttext.load_model(embedding_model)\n",
        "    vocab = dict([(word, model.get_word_id(word)) for word in model.get_words()])\n",
        "\n",
        "    content_index_list = []\n",
        "    onehot_labels_list = []\n",
        "    label_num_list = []\n",
        "    trainset_embedding_matrix = np.zeros((0, embed_size))\n",
        "    oov_word = []\n",
        "\n",
        "    count = 0\n",
        "\n",
        "   \n",
        "    with open(TRAIN_PATH, encoding='utf-8') as f:\n",
        "        for line in f.readlines():\n",
        "            df = json.loads(line)\n",
        "            content = df['value']\n",
        "\n",
        "            result = []\n",
        "\n",
        "            for item in word_tokenize(clean_str(content)):\n",
        "                word2id = vocab.get(item)\n",
        "                if word2id is None and item not in oov_word:\n",
        "                    oov_word.append(item)\n",
        "\n",
        "                    word_vec = model.get_word_vector(item)\n",
        "                    trainset_embedding_matrix = np.insert(trainset_embedding_matrix,\n",
        "                                                          len(trainset_embedding_matrix), values=word_vec, axis=0)\n",
        "                    word2id = len(model.get_words()) + count\n",
        "                    count += 1\n",
        "\n",
        "                elif word2id is None and item in oov_word:\n",
        "                    word2id = vocab_size + oov_word.index(item)\n",
        "\n",
        "                result.append(word2id)\n",
        "            content_index_list.append(result)\n",
        "\n",
        "            label_list = df[\"label_index\"]\n",
        "            num_label = df['label_number']\n",
        "\n",
        "            onehot_labels_list.append(create_onehot_labels(label_list, num_class))\n",
        "            label_num_list.append(num_label)\n",
        "    print('oov_word',len(oov_word),oov_word)\n",
        "  \n",
        "    with open('/content/trainset_embedding_matrix.npy', 'wb') as f:\n",
        "        np.save(f, trainset_embedding_matrix)\n",
        "    return content_index_list, onehot_labels_list, label_num_list,trainset_embedding_matrix,oov_word\n",
        "\n",
        "\n",
        "def test_data_word2vec(TEST_PATH,num_class,vocab_size,embedding_model,oov_word):\n",
        "    '''\n",
        "    create the testing set(test_x) and labels(test_y)\n",
        "\n",
        "    :param TEST_PATH: test dataset\n",
        "    :param num_class: total number of classes(rows + attribute)\n",
        "    :param vocab_size: number of total vocabulary in pretrained embedding\n",
        "    :param embedding_model: pretrained embedding model file\n",
        "    :param oov_word: oov word in training dataset\n",
        "\n",
        "    :return: content_index_list, word vector index in embedding matrix\n",
        "             onehot_labels_list, word label vector\n",
        "\n",
        "    '''\n",
        "\n",
        "\n",
        "    model = fasttext.load_model(embedding_model)\n",
        "    vocab = dict([(word, model.get_word_id(word)) for word in model.get_words()])\n",
        "\n",
        "    oov_vocab = dict([(word, oov_word.index(word)+vocab_size) for word in oov_word])\n",
        "    whole_vocab ={}\n",
        "    whole_vocab.update(vocab)\n",
        "    whole_vocab.update(oov_vocab)\n",
        "\n",
        "   \n",
        "\n",
        "    content_index_list = []\n",
        "    onehot_labels_list = []\n",
        "    label_number_list = []\n",
        "    oov_list = []\n",
        "\n",
        "    with open(TEST_PATH, encoding='utf-8') as f:\n",
        "        for line in f.readlines():\n",
        "\n",
        "            df = json.loads(line)\n",
        "            content = df['value']\n",
        "\n",
        "            result = []\n",
        "\n",
        "            for item in word_tokenize(clean_str(content)):\n",
        "                word2id = whole_vocab.get(item)\n",
        "                if word2id is None:\n",
        "\n",
        "                    word2id = 0\n",
        "                    oov_list.append(item)\n",
        "                result.append(word2id)\n",
        "            content_index_list.append(result)\n",
        "\n",
        "\n",
        "            label_list = df[\"label_index\"]\n",
        "            num_label = df['label_number']\n",
        "            onehot_labels_list.append(create_onehot_labels(label_list, num_class))\n",
        "            label_number_list.append(num_label)\n",
        "\n",
        "    \n",
        "    return content_index_list, onehot_labels_list,label_number_list\n",
        "\n",
        "\n",
        "def load_word2vec_matrix(embedding_model):\n",
        "    '''\n",
        "    create pretrained word embedding matrix\n",
        "\n",
        "    :param embedding_model: pretrained embedding model file\n",
        "\n",
        "    :return:\n",
        "             vocab_size, number of total vacabulary in pretrained embedding\n",
        "             embedding_size, embedding size for each word(word vector dimension)\n",
        "             embedding_matrix, word embedding matrix\n",
        "    '''\n",
        "\n",
        "\n",
        "    model = fasttext.load_model(embedding_model)\n",
        "    vocab_size = (model.get_output_matrix()).shape[0]\n",
        "    embedding_size = model.get_dimension()\n",
        "\n",
        "    vocab = dict([(word, model.get_word_id(word)) for word in model.get_words()])\n",
        "\n",
        "    embedding_matrix = np.zeros([vocab_size, embedding_size])\n",
        "    for word, index in vocab.items():\n",
        "        if word is not None:\n",
        "            embedding_matrix[index] = model[word]\n",
        "    with open('/content/embedding_matrix.npy', 'wb') as f:\n",
        "        np.save(f, embedding_matrix)\n",
        "    return vocab_size, embedding_size, embedding_matrix\n",
        "\n",
        "\n",
        "\n",
        "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
        "    '''\n",
        "\n",
        "    :param inputs: unbatched data\n",
        "    :param outputs: batched data\n",
        "    :param batch_size: size of every data batch\n",
        "    :param num_epochs: number of epochs\n",
        "\n",
        "    :return:\n",
        "           A batch iterator for data set\n",
        "    '''\n",
        "    inputs = np.array(inputs)\n",
        "    outputs = np.array(outputs)\n",
        "\n",
        "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
        "            yield inputs[start_index:end_index], outputs[start_index:end_index]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwjBTyFTcCza"
      },
      "source": [
        "\n",
        "def get_onehot_label_topk(scores, top_num,threshold = 0.1):\n",
        "    '''\n",
        "\n",
        "    get the top k score from testing result,\n",
        "    use threshold to filter the irrelevant data.\n",
        "\n",
        "    :param scores:  predicted scores for each classification class\n",
        "    :param top_num: number of labels for each data(corrosponding to top k scores)\n",
        "    :param threshold: score of irrelevant data < threshold\n",
        "\n",
        "    :return:\n",
        "        predicted_onehot_labels: Predict labels (onehot format)\n",
        "    '''\n",
        "\n",
        "    predicted_onehot_labels = []\n",
        "    score = np.ndarray.tolist(scores)\n",
        "    # for score in scores:\n",
        "    count = 0\n",
        "    onehot_labels_list = [0] * len(score)\n",
        "    max_num_index_list = list(map(score.index, heapq.nlargest(top_num, score)))\n",
        "    for index, predict_score in enumerate(score):\n",
        "        if predict_score >= threshold:\n",
        "\n",
        "            count += 1\n",
        "    if count < top_num:\n",
        "\n",
        "        onehot_labels_list[-3] = 1\n",
        "        onehot_labels_list[-2] = 1\n",
        "        onehot_labels_list[-1] = 1\n",
        "        predicted_onehot_labels.append(onehot_labels_list)\n",
        "    else:\n",
        "        for i in max_num_index_list:\n",
        "            onehot_labels_list[i] = 1\n",
        "        predicted_onehot_labels.append(onehot_labels_list)\n",
        "\n",
        "    return predicted_onehot_labels\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1CBTp6fUCp1"
      },
      "source": [
        "# using output label to assemble data into target table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFQTdp3YTj5M"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "column_label_dict = {\r\n",
        "                    '1':'Confirmed','2':'Deaths','3':'Recovered',\r\n",
        "                    '4':'retail_and_recreation_percent_change_from_baseline',\r\n",
        "                    '5':'grocery_and_pharmacy_percent_change_from_baseline',\r\n",
        "                    '6':'parks_percent_change_from_baseline',\r\n",
        "                    '7':'transit_stations_percent_change_from_baseline',\r\n",
        "                    '8':'workplaces_percent_change_from_baseline',\r\n",
        "                    '9':'residential_percent_change_from_baseline'\r\n",
        "}\r\n",
        "\r\n",
        "def onehot_to_index(onehot_label):\r\n",
        "    return list(np.where(onehot_label==1)[0]+1)\r\n",
        "\r\n",
        "def assembler(TRAIN_PATH,predicted_label,row_num,col_num):    \r\n",
        "    data = np.full((row_num,col_num+1),fill_value=' ',dtype=object)\r\n",
        "\r\n",
        "    with open(TRAIN_PATH, encoding='utf-8') as f:\r\n",
        "      for index,line in enumerate(f):\r\n",
        "          df = json.loads(line)\r\n",
        "          content = df['value'] \r\n",
        "          \r\n",
        "          position = onehot_to_index(predicted_label[index]) # return [column_label, row_label]\r\n",
        "        \r\n",
        "          column_label = str(column_label_dict[str(position[0])])\r\n",
        "          \r\n",
        "          data[position[1]-col_num][0] =  str(content['country_region']+','+\r\n",
        "                                      content['province_state']+','+content['county_subregion'])     # insert key(country-state-county value)\r\n",
        "          data[position[1]-col_num][position[0]] = content[column_label] # insert cell value\r\n",
        "\r\n",
        "    df = pd.DataFrame(data,\r\n",
        "                        columns=['country_region, province_state, county_subregion',\r\n",
        "                                 'Confirmed','Deaths','Recovered',\r\n",
        "                                'retail_and_recreation_percent_change_from_baseline',\r\n",
        "                                'grocery_and_pharmacy_percent_change_from_baseline',\r\n",
        "                                'parks_percent_change_from_baseline',\r\n",
        "                                'transit_stations_percent_change_from_baseline',\r\n",
        "                                'workplaces_percent_change_from_baseline',\r\n",
        "                                'residential_percent_change_from_baseline',\r\n",
        "                                ])\r\n",
        "    output_path = 'output_table.csv'\r\n",
        "\r\n",
        "    df.to_csv(output_path, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhhsh4eudhBJ"
      },
      "source": [
        "# Training and Testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSxd5e-xcEOG"
      },
      "source": [
        "\n",
        "\n",
        "def train(train_x, train_y, test_x, test_y,vocab_size,\n",
        "          embedding_size, pretrained_embedding, trainset_embedding_matrix):\n",
        "    '''\n",
        "    traing process + testing process\n",
        "\n",
        "    :param train_x: training dataset\n",
        "    :param train_y: training label\n",
        "    :param test_x: testing dataset\n",
        "    :param test_y: testing label\n",
        "    :param vocab_size: number of vocabulary in embedding matrx\n",
        "    :param embedding_size: embedding size for each word\n",
        "    :param pretrained_embedding: pretrained word embedding matrix\n",
        "    :param trainset_embedding_matrix: oov word(from train set) embedding matrix\n",
        "    :param args:\n",
        "\n",
        "    :return:\n",
        "            print testing result by fixed epoch interval\n",
        "    '''\n",
        "    label = []\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        model = WordRNN( MAX_DOCUMENT_LEN, NUM_CLASS,vocab_size = vocab_size,embedding_size= embedding_size,\n",
        "                         trainset_embedding=trainset_embedding_matrix)\n",
        "\n",
        "\n",
        "        # Define training procedure\n",
        "        global_step = tf.Variable(0, trainable=False)\n",
        "        params = tf.trainable_variables()\n",
        "        gradients = tf.gradients(model.loss, params)\n",
        "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "        optimizer = tf.train.AdamOptimizer(model.lr)\n",
        "        train_op = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=global_step)\n",
        "\n",
        "        # Initialize all variables\n",
        "        feed_dict_emb = {\n",
        "            model.x_init: np.float32(pretrained_embedding)\n",
        "        }\n",
        "        sess.run(tf.global_variables_initializer(),feed_dict=feed_dict_emb)\n",
        "\n",
        "\n",
        "        def train_step(batch_x, batch_y):\n",
        "            feed_dict = {\n",
        "                model.x: batch_x,\n",
        "                model.y: batch_y,\n",
        "                model.keep_prob: 0.8,\n",
        "\n",
        "            }\n",
        "            _, step, loss = sess.run([train_op, global_step, model.loss], feed_dict=feed_dict)\n",
        "\n",
        "            return loss\n",
        "\n",
        "        def test_accuracy(test_x, test_y):\n",
        "            '''\n",
        "\n",
        "            :param test_x: testing dataset\n",
        "            :param test_y: testing label\n",
        "            :return:\n",
        "                eval_loss: loss\n",
        "                accuracy: accuracy\n",
        "                ave_precison_score: average precison\n",
        "\n",
        "            '''\n",
        "\n",
        "\n",
        "            true_onehot_labels = []\n",
        "            predicted_onehot_scores = []\n",
        "\n",
        "\n",
        "            predicted_onehot_labels_t2 = []\n",
        "\n",
        "            test_batches = batch_iter(test_x, test_y, BATCH_SIZE, 1)\n",
        "            eval_loss, eval_counter = 0., 0\n",
        "\n",
        "\n",
        "            for test_batch_x, test_batch_y in test_batches:\n",
        "                scores, cur_loss = sess.run([model.scores, model.loss],\n",
        "                                            feed_dict={model.x: test_batch_x, model.y: test_batch_y,\n",
        "                                                       model.keep_prob: 1.0})\n",
        "\n",
        "                for labels,score in zip(test_batch_y,scores):\n",
        "                    true_onehot_labels.append(labels)\n",
        "                    predicted_onehot_scores.append(score)\n",
        "                \n",
        "                    number_label = list(labels).count(1)\n",
        "                   \n",
        "                    batch_predicted_onehot_labels = get_onehot_label_topk(scores=score, top_num=number_label)\n",
        "                    predicted_onehot_labels_t2.append(batch_predicted_onehot_labels[0])\n",
        "                   \n",
        "                eval_loss = eval_loss + cur_loss\n",
        "                eval_counter = eval_counter + 1\n",
        "\n",
        "\n",
        "             \n",
        "            #metrics\n",
        "            eval_loss = float(eval_loss / eval_counter)\n",
        "\n",
        "            accuracy = accuracy_score(np.array(true_onehot_labels), np.array(predicted_onehot_labels_t2))\n",
        "            roc_auc = roc_auc_score(np.array(true_onehot_labels), np.array(predicted_onehot_labels_t2), average = 'micro')\n",
        "            avg_precision = average_precision_score(np.array(true_onehot_labels), np.array(predicted_onehot_labels_t2),average = 'micro')\n",
        "            f1 = f1_score(np.array(true_onehot_labels), np.array(predicted_onehot_labels_t2),average='micro')\n",
        "            \n",
        "            ham = hamming_loss(np.array(true_onehot_labels), np.array(predicted_onehot_labels_t2))\n",
        "            \n",
        "            return eval_loss,avg_precision, accuracy,f1,roc_auc,ham,true_onehot_labels\n",
        "\n",
        "\n",
        "        # Training loop\n",
        "        start = time.time()\n",
        "\n",
        "        batches = batch_iter(train_x, train_y, BATCH_SIZE, NUM_EPOCHS)\n",
        "\n",
        "        st = time.time()\n",
        "        steps_per_epoch = int(num_train / BATCH_SIZE)\n",
        "        for batch_x, batch_y in batches:\n",
        "            step = tf.train.global_step(sess, global_step)\n",
        "            num_epoch = int(step / steps_per_epoch)\n",
        "\n",
        "            loss = train_step(batch_x, batch_y)\n",
        "\n",
        "            # if step % 400 == 0:\n",
        "            \n",
        "            \n",
        "            #     eval_loss,avg_precision, acc,f1,roc_auc,ham = test_accuracy(test_x, test_y)\n",
        "            \n",
        "            \n",
        "            #     print(\"epoch: {}, step: {}, loss: {}, steps_per_epoch: {}, batch size: {}\".\n",
        "            #          format(num_epoch, step, eval_loss, steps_per_epoch, BATCH_SIZE))\n",
        "            \n",
        "            #     print(\"avg_precision:{}, accuracy:{}, roc_auc: {},f1: {}, hamming_socre: {}\".format(avg_precision,acc,roc_auc,f1,ham))\n",
        "            #     print(\"time of one epoch: {}\\n\".format(time.time()-st))\n",
        "            #     st = time.time()\n",
        "\n",
        "        print('training time',time.time()-start)\n",
        "  \n",
        "  \n",
        "        test_start_time = time.time()\n",
        "        eval_loss,avg_precision, acc,f1,roc_auc,ham,label = test_accuracy(test_x, test_y)\n",
        "        print(\"avg_precision:{}, accuracy:{}, roc_auc: {},f1: {}, hamming_socre: {}\".format(avg_precision,acc,roc_auc,f1,ham))\n",
        "        print('testing time',time.time()-test_start_time)\n",
        "\n",
        "\n",
        "        return label\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rdn5OMLd4zj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b751f15-941a-4f66-a93e-f2d31bbb6c1b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFPZsSQSeGgf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b3b497-3a3c-47c3-f37a-931b444e171c"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import heapq\n",
        "import time\n",
        "\n",
        "from tflearn.data_utils import pad_sequences\n",
        "from sklearn.metrics import average_precision_score,accuracy_score,roc_auc_score,f1_score,hamming_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au-0hNnUSpjA"
      },
      "source": [
        "# Fasttext: generate word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGELBwGWego4"
      },
      "source": [
        "# !pip install fasttext\n",
        "# import fasttext\n",
        "# model = fasttext.train_unsupervised('/content/drive/MyDrive/Research/dataset/covid_19/corpus_covid-19_lowercase', 'skipgram',\n",
        "#                                     epoch=10, minn=2, maxn=5, dim=150, thread=16)\n",
        "# model.save_model('/content/corpus_covid_data.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5SStH5QjuzU"
      },
      "source": [
        "\n",
        "MODEL_PATH = '/content/corpus_covid_data.bin'\n",
        "TRAIN_PATH = \"/content/drive/MyDrive/Research/dataset/covid_19/covid-19_data_cell_level.json\"\n",
        "TEST_PATH = \"/content/drive/MyDrive/Research/dataset/covid_19/test_covid-19_data_cell_level.json\"\n",
        "with open(TEST_PATH, encoding='utf-8') as f:\n",
        "  num_Train = len(f.readlines())\n",
        "with open(TRAIN_PATH, encoding='utf-8') as f:\n",
        "  num_Test = len(f.readlines())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFlzPGaGeQcl"
      },
      "source": [
        "NUM_CLASS = 2864\n",
        "NUM_LABEL = 3\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 1\n",
        "MAX_DOCUMENT_LEN = 50\n",
        "num_train = num_Train\n",
        "num_test = num_Test #39456\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiReJCElcFe-"
      },
      "source": [
        "vocab_size, embedding_size, embedding_matrix = load_word2vec_matrix(MODEL_PATH)\n",
        "train_x_index_list, train_y, train_label_num_list,trainset_embedding_matrix, oov_word = train_data_word2vec(\n",
        "    TRAIN_PATH,NUM_CLASS,vocab_size,embedding_size, MODEL_PATH)\n",
        "train_x = pad_sequences(train_x_index_list, maxlen=MAX_DOCUMENT_LEN, value=0.)\n",
        "\n",
        "\n",
        "test_x_index_list, test_y, test_label_num_list= test_data_word2vec(TEST_PATH,NUM_CLASS, vocab_size,MODEL_PATH, oov_word)\n",
        "test_x = pad_sequences(test_x_index_list, maxlen=MAX_DOCUMENT_LEN, value=0.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "802IPqh1cGqf"
      },
      "source": [
        "%time predict_label = train(train_x, train_y, test_x, test_y,vocab_size,embedding_size, embedding_matrix,trainset_embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ5SvpTBrTWJ"
      },
      "source": [
        "# assemble data to output table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmocu4vFTxqG"
      },
      "source": [
        "assembler(TRAIN_PATH,predict_label,2810,9)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}