{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Bi-LSTM with Adversarial Training.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "96tUC8q1byy9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF9Mm8JEb9YD"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPFs3UslNHKl"
      },
      "source": [
        "# Model with adversarial training\n",
        "\n",
        "*   perturb = epsilon * gradient(input, loss)\n",
        "*   perturb_input = input + perturb\n",
        "\n",
        "*   self.scores: output prediction\n",
        "*   self.loss: loss by oringinal input\n",
        "*   self.perturb_loss: loss by perturbated input\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn1YbHmVb-kY"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn\n",
        "\n",
        "\n",
        "class WordRNN(object):\n",
        "    def __init__(self, max_document_length, num_class,vocab_size,embedding_size):\n",
        "        self.lr = 0.001\n",
        "        self.bag_num = 64\n",
        "         \n",
        "        # self.embedding_size = 256\n",
        "        self.num_hidden = 512\n",
        "        self.fc_num_hidden = 256\n",
        "\n",
        "        self.x = tf.placeholder(tf.int32, [None, max_document_length],name = 'input_x')\n",
        "        self.x_len = tf.reduce_sum(tf.sign(self.x), 1)\n",
        "        self.y = tf.placeholder(tf.float32, [None,num_class], name = 'input_y')\n",
        "        self.keep_prob = tf.placeholder(tf.float32, [],name = 'dropout')\n",
        "        self.x_init = tf.placeholder(tf.float32, shape=(vocab_size,embedding_size),name = 'x_init')\n",
        "        self.global_step = tf.Variable(0, trainable=False, name=\"Global_Step\")\n",
        "\n",
        "        self.adv_eps = tf.placeholder(tf.float32, shape=(),name = 'adv_eps') # adversarial training parameter eps\n",
        "        self.mask = mask = tf.placeholder(tf.float32, [None, max_document_length],name=\"mask\") # sentence mask\n",
        "        \n",
        "    def build(self,max_document_length,num_class,trainset_embedding):\n",
        "        self.shapes = shapes = tf.placeholder(tf.int32, [self.bag_num + 1])\n",
        "        self.lr = tf.Variable(0.001, name='learning_rate',trainable=False)\n",
        "        embeddings = tf.Variable(self.x_init, dtype=tf.float32,trainable=True,name=\"pretrained_embedding\")\n",
        "        train_embeddings = tf.Variable(trainset_embedding, dtype=tf.float32,\n",
        "                                        trainable=True,name=\"embs_only_in_train\")\n",
        "\n",
        "        embeddings = tf.concat([embeddings, train_embeddings], axis=0)\n",
        "        self.x_emb = input = tf.nn.embedding_lookup(embeddings, self.x)\n",
        "   \n",
        "        \n",
        "        def neural_net(x_emb, name = 'neural_net', reuse = False):\n",
        "            \n",
        "                \n",
        "            with tf.variable_scope(\"lstm\",reuse = reuse):\n",
        "                lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(self.num_hidden)  # forward direction cell\n",
        "                lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(self.num_hidden)  # backward direction cell\n",
        "                \n",
        "\n",
        "                rnn_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
        "                    lstm_fw_cell,lstm_bw_cell, x_emb, sequence_length=self.x_len, dtype=tf.float32)\n",
        "                \n",
        "\n",
        "                # Concat output\n",
        "                lstm_concat = tf.concat(rnn_outputs, axis=2)  # [batch_size, sequence_length, lstm_hidden_size * 2]\n",
        "                lstm_out = tf.reduce_mean(lstm_concat, axis=1)  # [batch_size, lstm_hidden_size * 2]\n",
        "\n",
        "                          \n",
        "                fc_output = tf.layers.dense(lstm_out, self.fc_num_hidden, activation=tf.nn.relu)\n",
        "                dropout = tf.nn.dropout(fc_output, self.keep_prob)\n",
        "  \n",
        "\n",
        "                W = tf.Variable(tf.truncated_normal(shape=[self.fc_num_hidden, num_class],\n",
        "                                                    stddev=0.1, dtype=tf.float32), name=\"W\")\n",
        "                b = tf.Variable(tf.constant(value=0.1, shape=[num_class], dtype=tf.float32), name=\"b\")\n",
        "                self.logits = tf.nn.xw_plus_b(dropout, W, b, name=\"logits\")\n",
        "                scores = tf.sigmoid(self.logits)\n",
        "               \n",
        "        \n",
        "                loss = tf.reduce_mean(tf.reduce_sum(\n",
        "                    tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.y),axis = 1))\n",
        "             \n",
        "                        \n",
        "\n",
        "            return scores, loss\n",
        "\n",
        "\n",
        "        self.scores, self.loss = neural_net(input, reuse=False)\n",
        "        \n",
        "        \n",
        "        ### adversarial training\n",
        "        raw_perturb = tf.gradients(self.loss, input)[0]  # [batch, L, dim]\n",
        "        print('input shape, {}'.format(input.shape[0]))\n",
        "        print('raw_perturb shape, {}'.format(raw_perturb.shape))\n",
        "        \n",
        "        # normalize the whole batch\n",
        "        \n",
        "        self.perturb = perturb = self.adv_eps * tf.stop_gradient(\n",
        "            tf.nn.l2_normalize(raw_perturb * tf.expand_dims(self.mask, axis=-1), dim=[0,1,2]))\n",
        "        \n",
        "        print('perturb shape, {}'.format(perturb.shape))\n",
        "        \n",
        "        self.perturb_inputs = perturb_inputs = input + perturb\n",
        "        self.perturb_probs, self.perturb_loss = neural_net(perturb_inputs, reuse=True)  # optimize the loss with perturbed loss\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "       \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeFgY6xAb-Db"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install fasttext\n",
        "!pip install tflearn\n",
        "!pip install sklearn\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S_3AMW6WG3z"
      },
      "source": [
        "# Util function\n",
        "*  load data \n",
        "*  generate embedding matrix\n",
        "*  generate batch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lISVY2p7cAXy"
      },
      "source": [
        "import fasttext\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "def clean_str(text):\n",
        "    '''\n",
        "    regular expression to clean text file\n",
        "\n",
        "    '''\n",
        "\n",
        "    text = re.sub(r\"[_{},:.!?%â€™\\'\\\"]\", \" \", str(text))\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", str(text))\n",
        "    text = text.strip().lower()\n",
        "    text = text.lower()\n",
        "    text = text.replace('[', '')\n",
        "    text = text.replace(']', '')\n",
        "\n",
        "    return text\n",
        "\n",
        "def create_onehot_labels(labels_index,num_labels):\n",
        "    '''\n",
        "    create onehot label vector\n",
        "\n",
        "    :param labels_index: preset order of label\n",
        "    :param num_labels: number of classes\n",
        "    :return: onehot label vector\n",
        "    '''\n",
        "    label = [0] * num_labels\n",
        "    for item in labels_index:\n",
        "\n",
        "        label[int(item-1)] = 1\n",
        "\n",
        "\n",
        "    return label\n",
        "\n",
        "\n",
        "def cos_sim(vector_a, vector_b):\n",
        "    \"\"\"\n",
        "    calculate the cosine similarity between two vectors\n",
        "\n",
        "    :param vector_a: vector a\n",
        "    :param vector_b: vector b\n",
        "    :return: cosine similary\n",
        "    \"\"\"\n",
        "\n",
        "    vector_a = np.mat(vector_a)\n",
        "    vector_b = np.mat(vector_b)\n",
        "    num = float(vector_a * vector_b.T)\n",
        "    denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
        "    sim = num / denom\n",
        "    return sim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_data_word2vec(TRAIN_PATH,num_class,vocab_size, embed_size, embedding_model):\n",
        "    '''\n",
        "    create the training set(train_x) and labels(train_y)\n",
        "\n",
        "    :param TRAIN_PATH: training data file\n",
        "    :param num_class: total number of classes(rows + attribute)\n",
        "    :param vocab_size: number of total vacabulary in pretrained embedding\n",
        "    :param embed_size: embedding size for each word(word vector dimension)\n",
        "    :param embedding_model: pretrained embedding model file\n",
        "    :return: content_index_list, word vector index in embedding matrix\n",
        "             onehot_labels_list, word label vector\n",
        "             trainset_embedding_matrix, oov word(from training dataset) embedding matrix\n",
        "             oov_word, oov word in training dataset\n",
        "    '''\n",
        "\n",
        "    model = fasttext.load_model(embedding_model)\n",
        "    vocab = dict([(word, model.get_word_id(word)) for word in model.get_words()])\n",
        "\n",
        "    content_index_list = []\n",
        "    onehot_labels_list = []\n",
        "    label_num_list = []\n",
        "    trainset_embedding_matrix = np.zeros((0, embed_size))\n",
        "    oov_word = []\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    # df = pd.read_csv(TRAIN_PATH, names=[\"content\", \"label1\", \"label2\",\"label3\"], sep=',', header=0,encoding='utf-8')\n",
        "    with open(os.path.join(folder_path,TRAIN_PATH), encoding='utf-8') as f:\n",
        "        for line in f.readlines():\n",
        "            df = json.loads(line)\n",
        "            content = df['value']\n",
        "\n",
        "            result = []\n",
        "\n",
        "            for item in word_tokenize(clean_str(content)):\n",
        "                word2id = vocab.get(item)\n",
        "                if word2id is None and item not in oov_word:\n",
        "                    oov_word.append(item)\n",
        "\n",
        "                    word_vec = model.get_word_vector(item)\n",
        "                    trainset_embedding_matrix = np.insert(trainset_embedding_matrix,\n",
        "                                                          len(trainset_embedding_matrix), values=word_vec, axis=0)\n",
        "                    word2id = len(model.get_words()) + count\n",
        "                    count += 1\n",
        "\n",
        "                elif word2id is None and item in oov_word:\n",
        "                    word2id = vocab_size + oov_word.index(item)\n",
        "\n",
        "                result.append(word2id)\n",
        "            content_index_list.append(result)\n",
        "\n",
        "            label_list = df[\"label_index\"]\n",
        "            num_label = df['label_number']\n",
        "\n",
        "            onehot_labels_list.append(create_onehot_labels(label_list, num_class))\n",
        "            label_num_list.append(num_label)\n",
        "    print('oov_word',len(oov_word),oov_word)\n",
        "   \n",
        "    with open('/content/trainset_embedding_matrix.npy', 'wb') as f:\n",
        "        np.save(f, trainset_embedding_matrix)\n",
        "    return content_index_list, onehot_labels_list, label_num_list,trainset_embedding_matrix,oov_word\n",
        "\n",
        "\n",
        "def test_data_word2vec(TEST_PATH,num_class,vocab_size,embedding_model,oov_word):\n",
        "    '''\n",
        "    create the testing set(test_x) and labels(test_y)\n",
        "\n",
        "    :param TEST_PATH: test dataset\n",
        "    :param num_class: total number of classes(rows + attribute)\n",
        "    :param vocab_size: number of total vocabulary in pretrained embedding\n",
        "    :param embedding_model: pretrained embedding model file\n",
        "    :param oov_word: oov word in training dataset\n",
        "\n",
        "    :return: content_index_list, word vector index in embedding matrix\n",
        "             onehot_labels_list, word label vector\n",
        "\n",
        "    '''\n",
        "\n",
        "\n",
        "    model = fasttext.load_model(embedding_model)\n",
        "    vocab = dict([(word, model.get_word_id(word)) for word in model.get_words()])\n",
        "\n",
        "    oov_vocab = dict([(word, oov_word.index(word)+vocab_size) for word in oov_word])\n",
        "    whole_vocab ={}\n",
        "    whole_vocab.update(vocab)\n",
        "    whole_vocab.update(oov_vocab)\n",
        "\n",
        "    # df = pd.read_csv(TEST_PATH, names=[ \"content\", \"label1\", \"label2\",\"label3\"], sep=',', header=0,encoding='utf-8')\n",
        "\n",
        "    content_index_list = []\n",
        "    onehot_labels_list = []\n",
        "    label_number_list = []\n",
        "    oov_list = []\n",
        "\n",
        "    with open(os.path.join(folder_path,TEST_PATH), encoding='utf-8') as f:\n",
        "        for line in f.readlines():\n",
        "\n",
        "            df = json.loads(line)\n",
        "            content = df['value']\n",
        "\n",
        "            result = []\n",
        "\n",
        "            for item in word_tokenize(clean_str(content)):\n",
        "                word2id = whole_vocab.get(item)\n",
        "                if word2id is None:\n",
        "\n",
        "                    word2id = 0\n",
        "                    oov_list.append(item)\n",
        "                result.append(word2id)\n",
        "            content_index_list.append(result)\n",
        "\n",
        "\n",
        "            label_list = df[\"label_index\"]\n",
        "            num_label = df['label_number']\n",
        "            onehot_labels_list.append(create_onehot_labels(label_list, num_class))\n",
        "            label_number_list.append(num_label)\n",
        "\n",
        "  \n",
        "    return content_index_list, onehot_labels_list,label_number_list\n",
        "\n",
        "\n",
        "def load_word2vec_matrix(embedding_model):\n",
        "    '''\n",
        "    create pretrained word embedding matrix\n",
        "\n",
        "    :param embedding_model: pretrained embedding model file\n",
        "\n",
        "    :return:\n",
        "             vocab_size, number of total vacabulary in pretrained embedding\n",
        "             embedding_size, embedding size for each word(word vector dimension)\n",
        "             embedding_matrix, word embedding matrix\n",
        "    '''\n",
        "\n",
        "\n",
        "    model = fasttext.load_model(embedding_model)\n",
        "    vocab_size = (model.get_output_matrix()).shape[0]\n",
        "    embedding_size = model.get_dimension()\n",
        "\n",
        "    vocab = dict([(word, model.get_word_id(word)) for word in model.get_words()])\n",
        "\n",
        "    embedding_matrix = np.zeros([vocab_size, embedding_size])\n",
        "    for word, index in vocab.items():\n",
        "        if word is not None:\n",
        "            embedding_matrix[index] = model[word]\n",
        "    with open('/content/embedding_matrix.npy', 'wb') as f:\n",
        "        np.save(f, embedding_matrix)\n",
        "    return vocab_size, embedding_size, embedding_matrix\n",
        "\n",
        "\n",
        "\n",
        "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
        "    '''\n",
        "\n",
        "    :param inputs: unbatched data\n",
        "    :param outputs: batched data\n",
        "    :param batch_size: size of every data batch\n",
        "    :param num_epochs: number of epochs\n",
        "\n",
        "    :return:\n",
        "           A batch iterator for data set\n",
        "    '''\n",
        "    inputs = np.array(inputs)\n",
        "    outputs = np.array(outputs)\n",
        "\n",
        "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
        "            yield inputs[start_index:end_index], outputs[start_index:end_index]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz_pqvZuWf_P"
      },
      "source": [
        "# Accuracy metrics\n",
        "*   top-k based\n",
        "*   threshold based"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwjBTyFTcCza"
      },
      "source": [
        "\n",
        "def get_onehot_label_topk(scores, top_num,threshold = 0.1):\n",
        "    '''\n",
        "\n",
        "    get the top k score from testing result,\n",
        "    use threshold to filter the irrelevant data.\n",
        "\n",
        "    :param scores:  predicted scores for each classification class\n",
        "    :param top_num: number of labels for each data(corrosponding to top k scores)\n",
        "    :param threshold: score of irrelevant data < threshold\n",
        "\n",
        "    :return:\n",
        "        predicted_onehot_labels: Predict labels (onehot format)\n",
        "    '''\n",
        "\n",
        "    predicted_onehot_labels = []\n",
        "    score = np.ndarray.tolist(scores)\n",
        "    # for score in scores:\n",
        "    count = 0\n",
        "    onehot_labels_list = [0] * len(score)\n",
        "    max_num_index_list = list(map(score.index, heapq.nlargest(top_num, score)))\n",
        "    for index, predict_score in enumerate(score):\n",
        "        if predict_score >= threshold:\n",
        "\n",
        "            count += 1\n",
        "    if count < top_num:\n",
        "\n",
        "        onehot_labels_list[-3] = 1\n",
        "        onehot_labels_list[-2] = 1\n",
        "        onehot_labels_list[-1] = 1\n",
        "        predicted_onehot_labels.append(onehot_labels_list)\n",
        "    else:\n",
        "        for i in max_num_index_list:\n",
        "            onehot_labels_list[i] = 1\n",
        "        predicted_onehot_labels.append(onehot_labels_list)\n",
        "\n",
        "    return predicted_onehot_labels\n",
        "\n",
        "def get_onehot_label_threshold(scores, threshold):\n",
        "    \"\"\"\n",
        "    Get the predicted one-hot labels based on the threshold.\n",
        "    If there is no predict score greater than threshold, then choose the label which has the max predict score.\n",
        "    Args:\n",
        "        scores: The all classes predicted scores provided by network.\n",
        "        threshold: The threshold (default: 0.5).\n",
        "    Returns:\n",
        "        predicted_onehot_labels: The predicted labels (one-hot).\n",
        "    \"\"\"\n",
        "    predicted_onehot_labels = []\n",
        "    score = np.ndarray.tolist(scores)\n",
        "    # for score in scores:\n",
        "    count = 0\n",
        "    onehot_labels_list = [0] * len(score)\n",
        "    for index, predict_score in enumerate(score):\n",
        "        if predict_score >= threshold:\n",
        "            onehot_labels_list[index] = 1\n",
        "            count += 1\n",
        "    if count == 0:\n",
        "        max_score_index = score.index(max(score))\n",
        "        onehot_labels_list[max_score_index] = 1\n",
        "    predicted_onehot_labels.append(onehot_labels_list)\n",
        "    return predicted_onehot_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCkxg34ZWwBI"
      },
      "source": [
        "# Training and Testing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSxd5e-xcEOG"
      },
      "source": [
        "\n",
        "def train(train_x, train_y, test_x, test_y,vocab_size,\n",
        "          embedding_size, pretrained_embedding, trainset_embedding_matrix):\n",
        "    '''\n",
        "    traing process + testing process\n",
        "\n",
        "    :param train_x: training dataset\n",
        "    :param train_y: training label\n",
        "    :param test_x: testing dataset\n",
        "    :param test_y: testing label\n",
        "    :param vocab_size: number of vocabulary in embedding matrx\n",
        "    :param embedding_size: embedding size for each word\n",
        "    :param pretrained_embedding: pretrained word embedding matrix\n",
        "    :param trainset_embedding_matrix: oov word(from train set) embedding matrix\n",
        "    :param args:\n",
        "\n",
        "    :return:\n",
        "            print testing result by fixed epoch interval\n",
        "    '''\n",
        "\n",
        "    \n",
        "    \n",
        "    model = WordRNN(MAX_DOCUMENT_LEN, NUM_CLASS,vocab_size = vocab_size,embedding_size= embedding_size)\n",
        "    model.build(MAX_DOCUMENT_LEN,NUM_CLASS,trainset_embedding=trainset_embedding_matrix)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        \n",
        "        \n",
        "        # Define training procedure\n",
        "        mask = np.zeros((BATCH_SIZE, MAX_DOCUMENT_LEN), dtype=np.float32)\n",
        "        global_step = tf.Variable(0,trainable=False)\n",
        "        optimizer = tf.train.AdamOptimizer(model.lr)\n",
        "    \n",
        "        \n",
        "        train_op = optimizer.minimize(model.perturb_loss, global_step=global_step)\n",
        "        \n",
        "       \n",
        "        # Initialize all variables\n",
        "        feed_dict_emb = {\n",
        "            model.x_init: np.float32(pretrained_embedding)\n",
        "        }\n",
        "        sess.run(tf.global_variables_initializer(),feed_dict=feed_dict_emb)\n",
        "\n",
        "\n",
        "        def train_step(batch_x, batch_y):\n",
        "            feed_dict = {\n",
        "                model.x: batch_x,\n",
        "                model.y: batch_y,\n",
        "                model.adv_eps: ADV_EPS,\n",
        "                model.mask: mask,\n",
        "                model.keep_prob: 0.8,\n",
        "\n",
        "            }\n",
        "            _, step, loss = sess.run([train_op, global_step, model.perturb_loss], feed_dict=feed_dict)\n",
        "\n",
        "            return loss\n",
        "\n",
        "        def test_accuracy(test_x, test_y):\n",
        "            '''\n",
        "\n",
        "            :param test_x: testing dataset\n",
        "            :param test_y: testing label\n",
        "            :return:\n",
        "                eval_loss: loss\n",
        "                accuracy: accuracy\n",
        "                ave_precison_score: average precison\n",
        "\n",
        "            '''\n",
        "\n",
        "\n",
        "            true_onehot_labels = []\n",
        "            predicted_onehot_scores = []\n",
        "\n",
        "\n",
        "            predicted_onehot_labels_t2 = []\n",
        "\n",
        "            test_batches = batch_iter(test_x, test_y, BATCH_SIZE, 1)\n",
        "            eval_loss, eval_counter = 0., 0\n",
        "\n",
        "\n",
        "            for test_batch_x, test_batch_y in test_batches:\n",
        "                scores, cur_loss = sess.run([model.scores, model.perturb_loss],\n",
        "                                            feed_dict={model.x: test_batch_x, model.y: test_batch_y,\n",
        "                                                       model.adv_eps: ADV_EPS,\n",
        "                                                       \n",
        "                                                       model.mask: mask,\n",
        "                                                       model.keep_prob: 1.0})\n",
        "\n",
        "                for labels,score in zip(test_batch_y,scores):\n",
        "                    true_onehot_labels.append(labels)\n",
        "                    predicted_onehot_scores.append(score)\n",
        "                    \n",
        "                    number_label = list(labels).count(1)\n",
        "                    \n",
        "                    #  using topk or threshold \n",
        "\n",
        "                    batch_predicted_onehot_labels = get_onehot_label_topk(scores=score, top_num=1)\n",
        "                    # batch_predicted_onehot_labels = get_onehot_label_threshold(scores=score,threshold = THRESHOLD)\n",
        "                    \n",
        "                    predicted_onehot_labels_t2.append(batch_predicted_onehot_labels[0])\n",
        "                    \n",
        "                eval_loss = eval_loss + cur_loss\n",
        "                eval_counter = eval_counter + 1\n",
        "\n",
        "            #metrics\n",
        "            eval_loss = float(eval_loss / eval_counter)\n",
        "\n",
        "            accuracy = accuracy_score(np.array(true_onehot_labels), np.array(predicted_onehot_labels_t2))\n",
        "            roc_auc = roc_auc_score(np.array(true_onehot_labels), np.array(predicted_onehot_labels_t2), average = 'micro')\n",
        "            avg_precision = average_precision_score(np.array(true_onehot_labels), np.array(predicted_onehot_labels_t2),average = 'micro')\n",
        "            precision = precision_score(np.array(true_onehot_labels), np.array(predicted_onehot_labels_t2),average = 'micro')\n",
        "            recall = recall_score(np.array(true_onehot_labels), np.array(predicted_onehot_labels_t2),average = 'micro')\n",
        "         \n",
        "            f1 = f1_score(np.array(true_onehot_labels), np.array(predicted_onehot_labels_t2),average='micro')\n",
        "            ham_loss = hamming_loss(np.array(true_onehot_labels), np.array(predicted_onehot_labels_t2))\n",
        "            \n",
        "            \n",
        "            return eval_loss,avg_precision, accuracy,precision,recall,f1,roc_auc,ham_loss\n",
        "\n",
        "\n",
        "        # Training loop\n",
        "        start = time.time()\n",
        "\n",
        "        batches = batch_iter(train_x, train_y, BATCH_SIZE, NUM_EPOCHS)\n",
        "\n",
        "        st = time.time()\n",
        "        steps_per_epoch = int(num_train / BATCH_SIZE)\n",
        "        \n",
        "        \n",
        "\n",
        "        for batch_x, batch_y in batches:\n",
        "            step = tf.train.global_step(sess, global_step)\n",
        "            num_epoch = int(step / steps_per_epoch)\n",
        "\n",
        "            loss = train_step(batch_x, batch_y)\n",
        "\n",
        "            if step % 300 == 0:\n",
        "            \n",
        "            \n",
        "                eval_loss,avg_precision, acc,precision,recall,f1,roc_auc,ham_loss = test_accuracy(test_x, test_y)\n",
        "            \n",
        "            \n",
        "                print(\"epoch: {}, step: {}, loss: {}, steps_per_epoch: {}, batch size: {}\".\n",
        "                    format(num_epoch, step, eval_loss, steps_per_epoch, BATCH_SIZE))\n",
        "            \n",
        "                print(\"avg_precision:{}, accuracy:{}, precision: {}, f1: {}, recall: {}\".format(avg_precision,acc,precision,f1,recall))\n",
        "                print(\"time of one epoch: {}\\n\".format(time.time()-st))\n",
        "                st = time.time()\n",
        "\n",
        "        print('training time',time.time()-start)\n",
        "        # #\n",
        "        test_start_time = time.time()\n",
        "        %time test_accuracy(test_x, test_y)\n",
        "        print('testing time',time.time()-test_start_time)\n",
        "        # print(eval_loss,acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rdn5OMLd4zj"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFPZsSQSeGgf"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import heapq\n",
        "import time\n",
        "\n",
        "from tflearn.data_utils import pad_sequences\n",
        "from sklearn.metrics import average_precision_score,accuracy_score,roc_auc_score,precision_score, recall_score, f1_score,hamming_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJs4yxj77JHf"
      },
      "source": [
        "# Taining the fasttext word emebdding model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGELBwGWego4"
      },
      "source": [
        "# !pip install fasttext\n",
        "# import fasttext\n",
        "# model = fasttext.train_unsupervised('/content/drive/MyDrive/Research/dataset/machine_log_data/corpus.txt', 'skipgram',\n",
        "#                                     epoch=10, minn=2, maxn=10, dim=150, thread=16)\n",
        "# model.save_model('/content/machine.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5SStH5QjuzU"
      },
      "source": [
        "import json\n",
        "import os \n",
        "MODEL_PATH = '/content/machine.bin'\n",
        "folder_path = '/content/drive/MyDrive/Research/dataset/key_index_based_dataset/cell_based_dataset/machine_log_data/no_numerical_value'\n",
        "TRAIN_PATH = \"training_machine_log_data_with_column_label_prediction.json\"\n",
        "TEST_PATH = \"testing_machine_log_data_with_column_label_prediction.json\"\n",
        "with open(os.path.join(folder_path,TRAIN_PATH), encoding='utf-8') as f:\n",
        "  num_Train = len(f.readlines())\n",
        "with open(os.path.join(folder_path,TEST_PATH), encoding='utf-8') as f:\n",
        "  num_Test = len(f.readlines())\n",
        "\n",
        "with open(os.path.join(folder_path,TRAIN_PATH), encoding='utf-8') as f1:\n",
        "  train_data_json = [json.loads(i) for i in f1.readlines()]\n",
        "\n",
        "\n",
        "dats = []\n",
        "for i in train_data_json:\n",
        "  dats.extend(i['label_index'])\n",
        "NUM_LABELS = len(set(dats))\n",
        "NUM_LABELS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFlzPGaGeQcl"
      },
      "source": [
        "NUM_CLASS = NUM_LABELS  \n",
        "\n",
        "BATCH_SIZE = 100\n",
        "NUM_EPOCHS = 100\n",
        "MAX_DOCUMENT_LEN = 30\n",
        "num_train = num_Train\n",
        "num_test = num_Test \n",
        "THRESHOLD = 0.5\n",
        "ADV_EPS = 0.05"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiReJCElcFe-"
      },
      "source": [
        "vocab_size, embedding_size, embedding_matrix = load_word2vec_matrix(MODEL_PATH)\n",
        "train_x_index_list, train_y, train_label_num_list,trainset_embedding_matrix, oov_word = train_data_word2vec(\n",
        "    TRAIN_PATH,NUM_CLASS,vocab_size,embedding_size, MODEL_PATH)\n",
        "train_x = pad_sequences(train_x_index_list, maxlen=MAX_DOCUMENT_LEN, value=0.)\n",
        "\n",
        "\n",
        "test_x_index_list, test_y, test_label_num_list= test_data_word2vec(TEST_PATH,NUM_CLASS, vocab_size,MODEL_PATH, oov_word)\n",
        "test_x = pad_sequences(test_x_index_list, maxlen=MAX_DOCUMENT_LEN, value=0.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "802IPqh1cGqf"
      },
      "source": [
        "%time train(train_x, train_y, test_x, test_y,vocab_size,embedding_size, embedding_matrix,trainset_embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXjMsZBGT_-E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}