{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Integration Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18-YmZQ-gKz8"
      },
      "source": [
        "## Install Deps\n",
        "- Transformer\n",
        "- einops\n",
        "- pytorch-lightning\n",
        "- neptune\n",
        "- gpytorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_HUgx_3VFVw"
      },
      "source": [
        "!pip install pytorch-lightning==1.2.8\n",
        "!pip install transformers neptune-client neptune-contrib gpytorch einops tflearn sklearn\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vMVxud8gJJh"
      },
      "source": [
        "## Download and prepare the dataset\n",
        "- Data_Integration_Dataset\n",
        " - Covid-19\n",
        " - Machine_Log"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7W2nWw4gBw3"
      },
      "source": [
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=19oLAKktjI0uk8v4lcdBTnRBTyqN-tGeR', output=None, quiet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KktZI2pLgFmZ"
      },
      "source": [
        "!unzip -qq Data_Integration_Dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwtDJt9hhB3P"
      },
      "source": [
        "import torch\n",
        "import einops\n",
        "import math\n",
        "import heapq\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "import pytorch_lightning.metrics.functional.accuracy as get_accuracy\n",
        "\n",
        "from pytorch_lightning.metrics.utils import select_topk\n",
        "from pytorch_lightning import Trainer, seed_everything\n",
        "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "from transformers import (\n",
        "    BertModel,\n",
        "    BertTokenizer,\n",
        "    AdamW,\n",
        "    get_cosine_with_hard_restarts_schedule_with_warmup,\n",
        "    AutoModel\n",
        "    )\n",
        "\n",
        "from torch.utils.data import (\n",
        "    random_split,\n",
        "    DataLoader,\n",
        "    RandomSampler,\n",
        "    Subset,\n",
        "    TensorDataset\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mhbl5RkqOdc"
      },
      "source": [
        "# Configuration\n",
        "\n",
        "## Hyperparameters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOUTY-GyqRMH"
      },
      "source": [
        "PRETRAINED_MODEL = 'bert-base-uncased'\n",
        "SAMPLE=10000\n",
        "CONTENT_FRAC = 0.20\n",
        "BATCH_SIZE = 128\n",
        "WARMUP = 2000\n",
        "MAX_CYCLES = 10\n",
        "MAX_EPOCHS = 60\n",
        "LEARNING_RATE = 0.00005\n",
        "\n",
        "SIGMOID_THRESHOLD = 0.5\n",
        "IS_BERT = False\n",
        "IS_MASK = True\n",
        "USE_CASE = 'Covid19' # 'Covid19' | 'MachineLog' \n",
        "EXP_TYPE = 'COL' # 'KEY' | 'COL' | 'AGGRE'(only for covid-19 dataset)\n",
        "\n",
        "CORE_TRANSFORMER_PARAMS = dict(\n",
        "    num_layers=12,\\\n",
        "    embedding_size=128,\\\n",
        "    layer_norm_epsilon=0.00001,\\\n",
        "    scale=0.01,\\\n",
        "    resid_pdrop=0.1,\\\n",
        "    attn_pdrop=0.1,\\\n",
        "    num_attention_heads = 8,\\\n",
        "    embd_pdrop=0.1,\n",
        "    num_actions=3,\\\n",
        "    common_conv_dim=128\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jgDm_50jh8E"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNQClG9pkFSM"
      },
      "source": [
        "def get_data(data_type=USE_CASE,exp_type=EXP_TYPE):\n",
        "  import json\n",
        "  import os\n",
        "  base_pth = None\n",
        "  train_pth = None\n",
        "  test_pth = None\n",
        "  if data_type == 'MachineLog':\n",
        "    base_pth = '/content/Data_Integration_Dataset/Machine_log'\n",
        "    if exp_type =='KEY':\n",
        "      train_pth = 'training_machine_log_data_with_key_index_prediction.json'\n",
        "      test_pth = 'testing_machine_log_data_with_key_index_prediction.json'\n",
        "    elif exp_type =='COL':\n",
        "      train_pth = 'training_machine_log_data_with_column_label_prediction.json'\n",
        "      test_pth = 'testing_machine_log_data_with_column_label_prediction.json'\n",
        "  \n",
        "  elif data_type == 'Covid19':\n",
        "    base_pth = '/content/Data_Integration_Dataset/Covid-19' \n",
        "   \n",
        "    if exp_type =='COL':\n",
        "       train_pth = 'training_covid-19_data_with_column_label_prediction.json'\n",
        "       test_pth = 'testing_covid-19_data_with_column_label_prediction.json'\n",
        "    elif exp_type =='KEY':\n",
        "      \n",
        "      train_pth = 'training_covid-19_data_with_key_index_prediction.json'\n",
        "      test_pth = 'testing_covid-19_data_with_key_index_prediction.json'\n",
        "    elif exp_type =='AGGRE':\n",
        "      train_pth = 'training_covid-19_data_with_aggregation_label_prediction.json'\n",
        "      test_pth = 'testing_covid-19_data_with_aggregation_label_prediction.json'\n",
        "      \n",
        "  if train_pth is None or test_pth is None:\n",
        "    raise Exception(\"dataset path not exist!\")\n",
        "\n",
        "  with open(os.path.join(base_pth,train_pth),'r',encoding='utf-8') as f:\n",
        "    train_data_json = [json.loads(i) for i in f.readlines()]\n",
        "  with open(os.path.join(base_pth,test_pth),'r',encoding='utf-8') as f:\n",
        "    test_data_json = [json.loads(i) for i in f.readlines()]\n",
        "  return train_data_json,test_data_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8ilMP3ij4Nd"
      },
      "source": [
        "train_data_json,test_data_json = get_data(data_type=USE_CASE,exp_type=EXP_TYPE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JogTVFZBt0Za"
      },
      "source": [
        "dats = []\n",
        "for i in train_data_json:\n",
        "  dats.extend(i['label_index'])\n",
        "fd={i:0 for i in dats}\n",
        "for i in dats:\n",
        "  fd[i]+=1\n",
        "\n",
        "NUM_LABELS = len(set(dats))+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md5PFoDagfE0"
      },
      "source": [
        "## Inititalize weights "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8hWJYMsi6wo"
      },
      "source": [
        "total = sum([fd[f] for f in fd])\n",
        "wt_init = [1/(fd[f]/total) for f in fd]\n",
        "WEIGHTS = [w/sum(wt_init) for w in wt_init]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qea7aT8Wjk5D"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otcVAzyunJjv"
      },
      "source": [
        "\n",
        "seed_everything()\n",
        "\n",
        "\n",
        "class DocumentDataPreprocessor():\n",
        "    \"\"\"DocumentDataPreprocessor \n",
        "    \"\"\"\n",
        "    CLASS_TOKEN = '[CLS]'\n",
        "    SEP_TOKEN = '[EOS]'\n",
        "    SPECIAL_TOKENS = []\n",
        "\n",
        "    def __init__(self,tokenizer:BertTokenizer,\\\n",
        "                column_split_order=[],\n",
        "                ):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def get_tokenized_text(self,content_text,max_length=1024,pad_to_max_length=True):\n",
        "        attention_mask = []\n",
        "        input_ids = []\n",
        "        encoded_dict = self.tokenizer.encode_plus(\n",
        "                    content_text,                      # Sentence to encode.\n",
        "                    add_special_tokens = True,         # Add '[CLS]' and '[SEP]'\n",
        "                    max_length = max_length,           # Pad & truncate all sentences.\n",
        "                    padding = 'max_length',\n",
        "                    truncation=True,\n",
        "                    return_attention_mask=True,\n",
        "                    return_tensors = 'pt',             # Return pytorch tensors.\n",
        "            )\n",
        "    \n",
        "        # Add the encoded sentence to the list.    \n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_mask.append(encoded_dict['attention_mask'])\n",
        "        # And its attention mask (simply differentiates padding from non-padding).\n",
        "        input_ids = torch.cat(input_ids,dim=0)\n",
        "        attention_mask = torch.cat(attention_mask,dim=0)\n",
        "        return input_ids,attention_mask\n",
        "    \n",
        "\n",
        "    @staticmethod\n",
        "    def split_dataset(dataset,train_percent=0.9):\n",
        "        # Create a split in train-validation \n",
        "        # Calculate the number of samples to include in each set.\n",
        "        if train_percent > 1:\n",
        "            raise Exception('Training Percentage cannot be > 1')\n",
        "        train_size = int(train_percent * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "\n",
        "        # Divide the dataset by randomly selecting samples.\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "        return train_dataset,val_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhxi1LfLkS7N"
      },
      "source": [
        "def create_training_set(data_json,tokenizer,exp_type=EXP_TYPE):\n",
        "  import json\n",
        "  proc = DocumentDataPreprocessor(tokenizer)\n",
        "  input_ids,att_mask,labels = [],[],[]\n",
        "  for d in data_json:\n",
        "    json_value = json.dumps(d['value'])\n",
        "    i,m = proc.get_tokenized_text(json_value,max_length=70)\n",
        "    input_ids.append(i)\n",
        "    att_mask.append(m)\n",
        "    if exp_type == 'KEY':\n",
        "        labels.append(d['label_index'][0])\n",
        "    elif exp_type == 'COL':\n",
        "      hot_tensor= F.one_hot(torch.LongTensor([r-1 for r in d['label_index']]),num_classes=NUM_LABELS).sum(dim=0)\n",
        "      labels.append(hot_tensor)\n",
        "\n",
        "  input_ids = torch.cat(input_ids,dim=0)\n",
        "  att_mask = torch.cat(att_mask,dim=0)\n",
        "  if exp_type == 'KEY':\n",
        "    labels = torch.Tensor(labels)\n",
        "  elif exp_type == 'COL':\n",
        "    labels = torch.stack(labels,dim=0)\n",
        " \n",
        " \n",
        "  return TensorDataset(input_ids,att_mask,labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPZIeMmSlXFZ"
      },
      "source": [
        "# Pretrained Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn5wAgeAmSMS"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)#prajjwal1/bert-tiny"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-zMQ-yqn_9t"
      },
      "source": [
        "train_dataset = create_training_set(train_data_json,tokenizer)\n",
        "train_dataset,val_dataset = DocumentDataPreprocessor.split_dataset(train_dataset)\n",
        "train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
        "val_loader = DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
        "\n",
        "test_dataset =  create_training_set(test_data_json,tokenizer)\n",
        "test_loader = DataLoader(test_dataset,batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MQRcM0OjpwM"
      },
      "source": [
        "# Model Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmEP-_TjmgH7"
      },
      "source": [
        "## Pretrained BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnIVgMQjl5D4"
      },
      "source": [
        "class PredictionHead(nn.Module):\n",
        "    def __init__(self, hidden_size=768,layer_norm_eps=0.00001,num_preds=NUM_LABELS):\n",
        "        super().__init__()\n",
        "        \n",
        "        # The output weights are the same as the input embeddings, but there is\n",
        "        # an output-only bias for each token.\n",
        "        self.decoder = nn.Linear(hidden_size, num_preds)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        \n",
        "        hidden_states = self.decoder(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "class BertDataIntegrationClassifier(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    self.pred_head = PredictionHead(hidden_size=self.model.config.hidden_size,layer_norm_eps=self.model.config.layer_norm_eps)\n",
        "    self.sfmx = nn.Softmax(dim=1)\n",
        "  \n",
        "  def forward(self,input,att_mask=None):\n",
        "    dx = self.model(input,attention_mask=att_mask)\n",
        "    val = dx.pooler_output\n",
        "    final_st = dx.pooler_output # val[:,0] # Getting classtoken op\n",
        "    return self.pred_head(final_st) #self.sfmx()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glTQjRf6mmpF"
      },
      "source": [
        "## Encoder-Only Vanilla Transformer "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE_FDGKrPyuW"
      },
      "source": [
        "### Vanilla Self Attention\n",
        "- NO bs of Other things like mask/cross attention etc.  \n",
        "- Attention defined by\n",
        "  - $$ W_Q,W_K,W_V: \\text{Are key,query,and value matrixes}$$\n",
        "  - $$attention = (softmax(W_Q \\times W_K)*scale) \\times W_V$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX1p3q-E_WbM"
      },
      "source": [
        "class SimpleSelfAttention(nn.Module):\n",
        "  '''\n",
        "  Vanilla Self attention on sequence. No Masking etc.\n",
        "  '''\n",
        "  def __init__(self,hidden_size,dropout=0.1,num_heads=4,scale=0.2,mlp_dim=3072):\n",
        "    super().__init__()\n",
        "    self.kqv_layer = nn.Linear(hidden_size,3*hidden_size)\n",
        "    self.num_heads = num_heads\n",
        "    self.ff_layer = nn.Sequential(\n",
        "          nn.Linear(hidden_size, hidden_size),\n",
        "          nn.Dropout(dropout)\n",
        "    )\n",
        "    \n",
        "    self.scale = hidden_size ** -scale\n",
        "\n",
        "  def forward(self,sequence_embedding):\n",
        "   \n",
        "    kqv = self.kqv_layer(sequence_embedding).chunk(3, dim = -1)\n",
        "    \n",
        "    k,q,v = map(lambda x:einops.rearrange(x,'b s (h d) -> b h s d',h=self.num_heads),kqv)\n",
        "    scaled_dot_product = torch.einsum('bhsd,bhnd->bhsn',k,q) * self.scale\n",
        "    weighted_sum = F.softmax(scaled_dot_product,dim=-1)\n",
        "    value_weighted_sum = torch.einsum('bhsn,bhsd->bhnd',weighted_sum,v)\n",
        "    reweighted_sequence_embedding = einops.rearrange(value_weighted_sum,'b h s d -> b s (h d)',h=self.num_heads)\n",
        "    return self.ff_layer(reweighted_sequence_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH1Z0Yxo_Ya5"
      },
      "source": [
        "\n",
        "class Conv1D(nn.Module):\n",
        "    \"\"\"\n",
        "    1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).\n",
        "    Basically works like a linear layer but the weights are transposed.\n",
        "    Args:\n",
        "        nf (:obj:`int`): The number of output features.\n",
        "        nx (:obj:`int`): The number of input features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nf, nx):\n",
        "        super().__init__()\n",
        "        self.nf = nf\n",
        "        w = torch.empty(nx, nf)\n",
        "        nn.init.normal_(w, std=0.02)\n",
        "        self.weight = nn.Parameter(w)\n",
        "        self.bias = nn.Parameter(torch.zeros(nf))\n",
        "\n",
        "    def forward(self, x):\n",
        "        size_out = x.size()[:-1] + (self.nf,)\n",
        "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
        "        x = x.view(*size_out)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bVe-5hKhs2w"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_state,embedding_size=256,resid_pdrop=0.1):  # in MLP: n_state=(n * embedding_size)\n",
        "        super().__init__()\n",
        "        nx = embedding_size # n_state = outputfeatures\n",
        "        self.c_fc = Conv1D(n_state, nx)\n",
        "        self.c_proj = Conv1D(nx, n_state) # nx = inputfeatures\n",
        "        self.act = nn.GELU()\n",
        "        self.dropout = nn.Dropout(resid_pdrop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.act(self.c_fc(x))\n",
        "        h2 = self.c_proj(h)\n",
        "        return self.dropout(h2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xL50OaNAjEW"
      },
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OenWOp8Ikte"
      },
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, \n",
        "                 embedding_size=256,\\\n",
        "                 layer_norm_epsilon=0.00001,\\\n",
        "                 scale=0.2,\\\n",
        "                 resid_pdrop=0.1,\\\n",
        "                 attn_pdrop=0.1,\\\n",
        "                 num_attention_heads = 8):\n",
        "        super().__init__()\n",
        "        hidden_size = embedding_size\n",
        "        inner_dim = 4 * hidden_size\n",
        "        self.ln_1 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
        "        self.attn = Residual(SimpleSelfAttention(hidden_size,num_heads = num_attention_heads,scale=scale,dropout=attn_pdrop,mlp_dim=inner_dim))\n",
        "        self.ln_2 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
        "        self.mlp = Residual(MLP(inner_dim,embedding_size=embedding_size,resid_pdrop=resid_pdrop))\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "    ):\n",
        "        attn_outputs = self.attn(\n",
        "            self.ln_1(hidden_states),\n",
        "        )\n",
        "        feed_forward_hidden_states = self.mlp(self.ln_2(attn_outputs))\n",
        "        return feed_forward_hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH3l1oVBBQKh"
      },
      "source": [
        "### Positional Embedding\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2s7AbDsIQXp"
      },
      "source": [
        "def make_positions(tensor, padding_idx, left_pad):\n",
        "    \"\"\"Replace non-padding symbols with their position numbers.\n",
        "    Position numbers begin at padding_idx+1.\n",
        "    Padding symbols are ignored, but it is necessary to specify whether padding\n",
        "    is added on the left side (left_pad=True) or right side (left_pad=False).\n",
        "    \"\"\"\n",
        "    max_pos = padding_idx + 1 + tensor.size(1)\n",
        "    device = tensor.get_device()\n",
        "    buf_name = f'range_buf_{device}'\n",
        "    if not hasattr(make_positions, buf_name):\n",
        "        setattr(make_positions, buf_name, tensor.new())\n",
        "    setattr(make_positions, buf_name, getattr(make_positions, buf_name).type_as(tensor))\n",
        "    if getattr(make_positions, buf_name).numel() < max_pos:\n",
        "        torch.arange(padding_idx + 1, max_pos, out=getattr(make_positions, buf_name))\n",
        "    mask = tensor.ne(padding_idx)\n",
        "    positions = getattr(make_positions, buf_name)[:tensor.size(1)].expand_as(tensor)\n",
        "    if left_pad:\n",
        "        positions = positions - mask.size(1) + mask.long().sum(dim=1).unsqueeze(1)\n",
        "    new_tensor = tensor.clone()\n",
        "    return new_tensor.masked_scatter_(mask, positions[mask]).long()\n",
        "\n",
        "\n",
        "class SinusoidalPositionalEmbedding(nn.Module):\n",
        "    \"\"\"This module produces sinusoidal positional embeddings of any length.\n",
        "    Padding symbols are ignored, but it is necessary to specify whether padding\n",
        "    is added on the left side (left_pad=True) or right side (left_pad=False).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim, padding_idx=0, left_pad=0, init_size=128):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.padding_idx = padding_idx\n",
        "        self.left_pad = left_pad\n",
        "        self.weights = dict()   # device --> actual weight; due to nn.DataParallel :-(\n",
        "        self.register_buffer('_float_tensor', torch.FloatTensor(1))\n",
        "\n",
        "    @staticmethod\n",
        "    def get_embedding(num_embeddings, embedding_dim, padding_idx=None):\n",
        "        \"\"\"Build sinusoidal embeddings.\n",
        "        This matches the implementation in tensor2tensor, but differs slightly\n",
        "        from the description in Section 3.5 of \"Attention Is All You Need\".\n",
        "        \"\"\"\n",
        "        half_dim = embedding_dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)\n",
        "        emb = torch.arange(num_embeddings, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)\n",
        "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(num_embeddings, -1)\n",
        "        if embedding_dim % 2 == 1:\n",
        "            # zero pad\n",
        "            emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)\n",
        "        if padding_idx is not None:\n",
        "            emb[padding_idx, :] = 0\n",
        "        return emb\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
        "        bsz, seq_len = input.size()\n",
        "        max_pos = self.padding_idx + 1 + seq_len\n",
        "        device = input.get_device()\n",
        "        if device not in self.weights or max_pos > self.weights[device].size(0):\n",
        "            # recompute/expand embeddings if needed\n",
        "            self.weights[device] = SinusoidalPositionalEmbedding.get_embedding(\n",
        "                max_pos,\n",
        "                self.embedding_dim,\n",
        "                self.padding_idx,\n",
        "            )\n",
        "        self.weights[device] = self.weights[device].type_as(self._float_tensor)\n",
        "        positions = make_positions(input, self.padding_idx, self.left_pad)\n",
        "        return self.weights[device].index_select(0, positions.view(-1)).view(bsz, seq_len, -1).detach()\n",
        "\n",
        "    def max_positions(self):\n",
        "        \"\"\"Maximum number of supported positions.\"\"\"\n",
        "        return int(1e5)  # an arbitrary large number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpwoxTUEHM17"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am2p5ifknJVu"
      },
      "source": [
        "class VanillaTransformer(nn.Module):\n",
        "  '''\n",
        "  Contains Sinusoidal Embedding for sequences as part of the Framework. \n",
        "  '''\n",
        "  def __init__(self,\n",
        "              num_layers = 4,\n",
        "              embed_dropout =0.1 ,\\\n",
        "              embedding_size=256,\\\n",
        "              layer_norm_epsilon=0.00001,\\\n",
        "              scale=0.2,\\\n",
        "              resid_pdrop=0.1,\\\n",
        "              attn_pdrop=0.1,\\\n",
        "              num_attention_heads = 8):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([])\n",
        "    self.embed_dropout=embed_dropout\n",
        "    self.embed_scale = math.sqrt(embedding_size)\n",
        "    self.embed_positions = SinusoidalPositionalEmbedding(embedding_size)\n",
        "\n",
        "    for _ in range(num_layers): \n",
        "        self.layers.append(Block(\n",
        "                          embedding_size=embedding_size,\\\n",
        "                          layer_norm_epsilon=layer_norm_epsilon,\\\n",
        "                          scale=scale,\\\n",
        "                          resid_pdrop=resid_pdrop,\\\n",
        "                          attn_pdrop=attn_pdrop,\\\n",
        "                          num_attention_heads = num_attention_heads))\n",
        "\n",
        "  def forward(self, x, mask = None):\n",
        "    # Add positional embedding\n",
        "    x = self.embed_scale * x # (b,len,d)\n",
        "    if self.embed_positions is not None:\n",
        "        x += self.embed_positions(x[:, :, 0])   \n",
        "    x = F.dropout(x, p=self.embed_dropout, training=self.training)\n",
        "\n",
        "    hidden = x\n",
        "    for attention_block in self.layers:\n",
        "        hidden = attention_block(hidden)\n",
        "    return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN-qQp1njvSV"
      },
      "source": [
        "## Ground Up Transformer with Bert Tokenized Embedding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dquqL1B7PNOb"
      },
      "source": [
        "class BertEmbedTransformer(nn.Module):\n",
        "  def __init__(self,\n",
        "              num_layers=8,\\\n",
        "              embedding_size=128,\\\n",
        "              layer_norm_epsilon=0.00001,\\\n",
        "              scale=0.01,\\\n",
        "              resid_pdrop=0.1,\\\n",
        "              attn_pdrop=0.1,\\\n",
        "              num_attention_heads = 8,\\\n",
        "              embd_pdrop=0.1,\n",
        "              num_actions=3,\\\n",
        "              common_conv_dim=64):\n",
        "    super().__init__()\n",
        "    data_input = dict(\n",
        "        embedding_size=embedding_size,\n",
        "        num_layers =num_layers,\n",
        "        layer_norm_epsilon =layer_norm_epsilon,\n",
        "        scale =scale,\n",
        "        resid_pdrop =resid_pdrop,\n",
        "        attn_pdrop =attn_pdrop,\n",
        "        num_attention_heads =num_attention_heads,\n",
        "    )\n",
        "    \n",
        "    self.transformer = VanillaTransformer(**data_input)\n",
        "    bert_model = AutoModel.from_pretrained(PRETRAINED_MODEL)\n",
        "    bert_emb = bert_model.embeddings.word_embeddings\n",
        "    text_embedding_dim = bert_emb.embedding_dim\n",
        "    num_emb = bert_emb.num_embeddings\n",
        "    self.text_embeddings = nn.Embedding(num_embeddings=num_emb,embedding_dim=text_embedding_dim)\n",
        "    \n",
        "    self.text_embeddings.load_state_dict(bert_emb.state_dict())\n",
        "\n",
        "    self.text_embeddings.weight.requires_grad = False\n",
        "    self.text_conv = nn.Conv1d(text_embedding_dim,common_conv_dim,kernel_size=1,padding=0,bias=False)\n",
        "    self.text_cls_token = nn.Parameter(torch.randn(1, 1, common_conv_dim))\n",
        "    self.to_cls = nn.Identity()\n",
        "    self.final_layer = nn.Sequential(\n",
        "        nn.Linear(common_conv_dim, common_conv_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(embd_pdrop),\n",
        "        nn.Linear(common_conv_dim, common_conv_dim)\n",
        "    )\n",
        "    \n",
        "  def forward(self,inputs,att_mask=None):\n",
        "    text_tensor = self.text_embeddings(inputs)\n",
        "    \n",
        "    text_tensor = text_tensor.transpose(1,2)\n",
        "\n",
        "    text_tensor = self.text_conv(text_tensor).permute(0,2,1)\n",
        "    \n",
        "    # Prepend CLS tokens and Finally extract thoose instead of the last token.\n",
        "    b,n,_ = text_tensor.size()\n",
        "    text_cls_token = einops.repeat(self.text_cls_token,'() n d -> b n d', b = b)\n",
        "\n",
        "    text_tensor = torch.cat((text_cls_token,text_tensor),dim=1)\n",
        "    # adding Extra one for cls tokens that get prepended the tensors\n",
        "    if att_mask!=None:\n",
        "      att_mask = torch.cat((torch.ones(b).unsqueeze(1).to(text_tensor.device),att_mask),dim=1)\n",
        "    \n",
        "    text_j_tensor = self.transformer(text_tensor,mask=att_mask)\n",
        "    \n",
        "    l_txt = self.to_cls(text_j_tensor[:,0])\n",
        "    \n",
        "    # A residual block\n",
        "    concat_tensor_proj = self.final_layer(l_txt)\n",
        "    concat_tensor_proj+=l_txt\n",
        "    \n",
        "    return concat_tensor_proj \n",
        "    \n",
        "\n",
        "class TransformerDIClassifier(torch.nn.Module):\n",
        "  def __init__(self,\n",
        "              num_layers=8,\\\n",
        "              embedding_size=64,\\\n",
        "              layer_norm_epsilon=0.00001,\\\n",
        "              scale=0.01,\\\n",
        "              resid_pdrop=0.1,\\\n",
        "              attn_pdrop=0.1,\\\n",
        "              num_attention_heads = 8,\\\n",
        "              embd_pdrop=0.1,\n",
        "              num_actions=3,\\\n",
        "              common_conv_dim=64):\n",
        "    super().__init__()\n",
        "    self.model = BertEmbedTransformer(\n",
        "        num_layers = num_layers,\n",
        "        embedding_size = embedding_size,\n",
        "        layer_norm_epsilon = layer_norm_epsilon,\n",
        "        scale = scale,\n",
        "        resid_pdrop = resid_pdrop,\n",
        "        attn_pdrop = attn_pdrop,\n",
        "        num_attention_heads = num_attention_heads,\n",
        "        embd_pdrop = embd_pdrop,\n",
        "        num_actions = num_actions,\n",
        "        common_conv_dim = common_conv_dim,\n",
        "    )\n",
        "    self.pred_head = PredictionHead(hidden_size=common_conv_dim,layer_norm_eps=layer_norm_epsilon)\n",
        "    \n",
        "  \n",
        "  def forward(self,input,att_mask=None):\n",
        "    final_st = self.model(input,att_mask=att_mask)\n",
        "    return self.pred_head(final_st)  #self.sfmx()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30c3eibUaxip"
      },
      "source": [
        "## Lightning Module\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVbr2PG7PKhD"
      },
      "source": [
        "class DataIntegrationClassifier(pl.LightningModule):\n",
        "  def __init__(self,with_mask=False,is_bert=IS_BERT):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.with_mask = with_mask\n",
        "    if is_bert:\n",
        "      print(\"Using BERT Model\")\n",
        "      self.model = BertDataIntegrationClassifier()\n",
        "    else:\n",
        "      print(\"Using Vanilla Backbone Model\")\n",
        "      self.model = TransformerDIClassifier(**CORE_TRANSFORMER_PARAMS)\n",
        "  \n",
        "    \n",
        "    self.sfmx = nn.Softmax(dim=1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.loss = nn.BCEWithLogitsLoss()\n",
        "    self.sigmoid_threshold = SIGMOID_THRESHOLD\n",
        "    \n",
        "  def forward(self,inputs,att_mask=None): \n",
        "    '''\n",
        "    inputs : (input_ids)\n",
        "        - input_ids: b k :\n",
        "          - b: batchsize\n",
        "          - k: sequence_length\n",
        "    *_mask = mask: b s : binary tensor. \n",
        "    '''\n",
        "    return self.model(inputs,att_mask=att_mask)\n",
        "\n",
        "  \n",
        "  def get_topk_accuracy(self, y_pred, y_true,topk=1):\n",
        "    ## single label prediction task\n",
        "\n",
        "    pred_vals = select_topk(self.sigmoid(y_pred))\n",
        " \n",
        "    num_vals = pred_vals.size()[0]\n",
        "    correct_vals = 0  # Exactly Correct Vals\n",
        "    \n",
        "    index_list = []\n",
        "    for v1, v2 in zip(pred_vals, y_true):\n",
        "            \n",
        "        \n",
        "        if str(v1.cpu().numpy()) == str(v2.cpu().numpy()):\n",
        "            correct_vals += 1\n",
        "    \n",
        "    \n",
        "    acc = torch.tensor(correct_vals / num_vals)\n",
        "    precision = torch.tensor(correct_vals / num_vals)\n",
        "    return torch.tensor(correct_vals / num_vals)\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "  def training_step(self,batch,batch_nb):\n",
        "    inps,mask,labels = batch\n",
        "    \n",
        "    if not self.with_mask:\n",
        "      logits = self(inps,att_mask=None)\n",
        "    else:\n",
        "      logits = self(inps,att_mask=mask)\n",
        "    loss = self.loss(logits,labels.type_as(logits))\n",
        "    \n",
        "    \n",
        "    logit  = (self.sigmoid(logits) > 0.5).long()\n",
        "    accuracy = self.get_topk_accuracy(logits,labels)\n",
        "    \n",
        "    \n",
        "    precision = precision_score(logit.detach().cpu().numpy(),labels.detach().cpu().numpy(),average = 'micro')\n",
        "    recall = recall_score(logit.detach().cpu().numpy(),labels.detach().cpu().numpy(),average = 'micro')\n",
        "    f1 = f1_score(logit.detach().cpu().numpy(),labels.detach().cpu().numpy(),average = 'micro')\n",
        "    \n",
        "    self.logger.log_metrics({\n",
        "        'train_accuracy':accuracy.detach().cpu().numpy(),\n",
        "        'train_precision':precision,\n",
        "        'train_recall':recall,\n",
        "        'train_f1':f1,\n",
        "        'train_loss':loss.detach().cpu().numpy(),\n",
        "        'epoch': self.current_epoch,\n",
        "    })\n",
        "    return {'loss':loss}\n",
        "\n",
        "\n",
        "  def validation_step(self,batch,batch_nb):\n",
        "    inps,mask,labels = batch\n",
        "   \n",
        "    if not self.with_mask:\n",
        "      logits = self(inps,att_mask=None)\n",
        "    else:\n",
        "      logits = self(inps,att_mask=mask)\n",
        "    \n",
        "    loss = self.loss(logits,labels.type_as(logits))\n",
        "    \n",
        "    logit  = (self.sigmoid(logits) > 0.5).long()\n",
        "    precision = precision_score(logit.detach().cpu().numpy(),labels.detach().cpu().numpy(),average = 'micro')\n",
        "    recall = recall_score(logit.detach().cpu().numpy(),labels.detach().cpu().numpy(),average = 'micro')\n",
        "    f1 = f1_score(logit.detach().cpu().numpy(),labels.detach().cpu().numpy(),average = 'micro')\n",
        "    \n",
        "    accuracy = self.get_topk_accuracy(logits,labels)\n",
        "    self.logger.log_metrics({\n",
        "        'val_accuracy':accuracy.detach().cpu().numpy(),\n",
        "        'val_precision':precision,\n",
        "        'val_recall':recall,\n",
        "        'val_f1':f1,\n",
        "        'val_loss':loss.detach().cpu().numpy(),\n",
        "        'epoch': self.current_epoch,\n",
        "    })\n",
        "    return {'loss':loss,'val_loss':loss}\n",
        "  \n",
        "  def test_step(self,batch,batch_nb):\n",
        "    inps,mask,labels = batch\n",
        " \n",
        "    if not self.with_mask:\n",
        "      logits = self(inps,att_mask=None)\n",
        "    else:\n",
        "      logits = self(inps,att_mask=mask)\n",
        "    loss = self.loss(logits,labels.type_as(logits))\n",
        "    \n",
        "    logit  = (self.sigmoid(logits) > 0.5).long()\n",
        "    logit_numpy = logit.cpu().numpy()\n",
        "    with open('predicted_logits.txt','ab') as f:\n",
        "      np.savetxt(f,logit_numpy)\n",
        "    precision = precision_score(logit.detach().cpu().numpy(),labels.detach().cpu().numpy(),average = 'micro')\n",
        "    recall = recall_score(logit.detach().cpu().numpy(),labels.detach().cpu().numpy(),average = 'micro')\n",
        "    f1 = f1_score(logit.detach().cpu().numpy(),labels.detach().cpu().numpy(),average = 'micro')\n",
        "    clas_rep = classification_report(logit.detach().cpu().numpy(),labels.detach().cpu().numpy())\n",
        "    print(clas_rep)\n",
        "    \n",
        "    \n",
        "    \n",
        "    accuracy = self.get_topk_accuracy(logits,labels)\n",
        "    self.logger.log_metrics({\n",
        "        'test_accuracy':accuracy.detach().cpu().numpy(),\n",
        "        'test_precision':precision,\n",
        "        'test_recall':recall,\n",
        "        'test_f1':f1,\n",
        "        'test_loss':loss.detach().cpu().numpy(),\n",
        "        'epoch': self.current_epoch,\n",
        "    })\n",
        "    return {'loss':loss,'test_accuracy':accuracy}\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer =  AdamW(self.parameters(), lr=LEARNING_RATE, eps=1e-12, betas=(0.9,0.999))\n",
        "    num_minibatch_steps = NUM_TRAIN_SAMPLES/BATCH_SIZE\n",
        "    max_epochs = MAX_EPOCHS\n",
        "    warmup = WARMUP\n",
        "    t_total = max_epochs * num_minibatch_steps\n",
        "    num_cycles = MAX_CYCLES\n",
        "    lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, warmup, t_total,num_cycles=num_cycles)\n",
        "    return [optimizer] ,[lr_scheduler]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUyZXnk1_Tai"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVkRK2Uelupr"
      },
      "source": [
        "### Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QQ2tUeP_bA1"
      },
      "source": [
        "model = DataIntegrationColumnClassifier(with_mask=IS_MASK,is_bert=IS_BERT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RftpPFlPja8O"
      },
      "source": [
        "### Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkaFu1RW8svY"
      },
      "source": [
        "NUM_TRAIN_SAMPLES = len(train_dataset)\n",
        "# Instantiate ModelCheckpoint\n",
        "model_checkpoint = ModelCheckpoint(filename='model/checkpoints/{epoch:02d}-{val_loss:.2f}',\n",
        "                                   save_weights_only=True,\n",
        "                                   save_top_k=3,\n",
        "                                   monitor='val_loss',\n",
        "                                   period=1)\n",
        "trainer = Trainer(\n",
        "    automatic_optimization=True,\n",
        "    max_epochs=MAX_EPOCHS,\\\n",
        "    progress_bar_refresh_rate=25,\\\n",
        "    gpus=1,\\\n",
        "    checkpoint_callback=model_checkpoint\n",
        ")\n",
        "\n",
        "\n",
        "trainer.fit(model, train_loader,val_dataloaders=val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbQnxk91jTNx"
      },
      "source": [
        "### Testing process "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aTsbmXX2uuy"
      },
      "source": [
        "trainer.test(model, test_dataloaders=DataLoader(test_dataset,batch_size=1024,shuffle=False))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}