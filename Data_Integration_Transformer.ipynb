{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Upload-version Copy of Data-Integration-Redone-new-one.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Qea7aT8Wjk5D"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18-YmZQ-gKz8"
      },
      "source": [
        "## Install Deps\n",
        "- Transformer\n",
        "- einops\n",
        "- pytorch-lightning\n",
        "- neptune\n",
        "- gpytorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_HUgx_3VFVw"
      },
      "source": [
        "!pip install transformers pytorch-lightning neptune-client neptune-contrib gpytorch einops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrUOLAKPXUqJ",
        "outputId": "5a40ad73-ccad-484a-ef57-212f13241949"
      },
      "source": [
        "!pip uninstall torchtext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping torchtext as it is not installed.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hYcYmjAYVL3",
        "outputId": "95fcaa6b-012d-4168-c245-cc7a484b5bd2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mhbl5RkqOdc"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOUTY-GyqRMH"
      },
      "source": [
        "PRETRAINED_MODEL = 'bert-base-uncased'\n",
        "SAMPLE=10000\n",
        "CONTENT_FRAC = 0.20\n",
        "BATCH_SIZE = 512\n",
        "WARMUP = 2000\n",
        "MAX_CYCLES = 10\n",
        "MAX_EPOCHS = 75\n",
        "LEARNING_RATE = 0.005\n",
        "\n",
        "SIGMOID_THRESHOLD = 0.5\n",
        "IS_BERT = False\n",
        "IS_MASK = True\n",
        "USE_CASE = 'MachineLog' # 'Covid19' | 'MachineLog' | 'MachineLog_no_numerical'\n",
        "EXP_TYPE = 'COL'# 'KEY' | 'KEY+COL' | 'COL'\n",
        "\n",
        "CORE_TRANSFORMER_PARAMS = dict(\n",
        "    num_layers=12,\\\n",
        "    embedding_size=128,\\\n",
        "    layer_norm_epsilon=0.00001,\\\n",
        "    scale=0.01,\\\n",
        "    resid_pdrop=0.1,\\\n",
        "    attn_pdrop=0.1,\\\n",
        "    num_attention_heads = 8,\\\n",
        "    embd_pdrop=0.1,\n",
        "    num_actions=3,\\\n",
        "    common_conv_dim=128\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jgDm_50jh8E"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNQClG9pkFSM"
      },
      "source": [
        "def get_data(data_type=USE_CASE,exp_type=EXP_TYPE):\n",
        "  import json\n",
        "  import os\n",
        "  base_pth = None\n",
        "  train_pth = None\n",
        "  test_pth = None\n",
        "  if data_type == 'MachineLog':\n",
        "    base_pth = '/content/drive/MyDrive/Research/dataset/key_index_based_dataset/cell_based_dataset/machine_log_data'\n",
        "    if exp_type =='KEY':\n",
        "      train_pth = 'training_machine_log_data_with_key_index_prediction.json'\n",
        "      test_pth = 'testing_machine_log_data_with_key_index_prediction.json'\n",
        "    elif exp_type =='COL':\n",
        "      train_pth = 'training_machine_log_data_with_column_label_prediction.json'\n",
        "      test_pth = 'testing_machine_log_data_with_column_label_prediction.json'\n",
        "  elif data_type == 'MachineLog_no_numerical':\n",
        "    base_pth = '/content/drive/MyDrive/Research/dataset/key_index_based_dataset/cell_based_dataset/machine_log_data/no_numerical_value'\n",
        "    if exp_type =='KEY':\n",
        "      train_pth = 'training_machine_log_data_with_key_label_prediction.json'\n",
        "      test_pth = 'testing_machine_log_data_with_key_label_prediction.json'\n",
        "    elif exp_type =='COL':\n",
        "      train_pth = 'training_machine_log_data_with_column_label_prediction.json'\n",
        "      test_pth = 'testing_machine_log_data_with_column_label_prediction.json'\n",
        "  \n",
        "  elif data_type == 'Covid19':\n",
        "    base_pth = '/content/drive/MyDrive/Research/dataset/key_index_based_dataset/cell_based_dataset/covid-19_data'\n",
        "    if exp_type =='KEY':\n",
        "      train_pth = 'training_covid-19_data_with_key_index_prediction.json'\n",
        "      test_pth = 'testing_covid-19_data_with_key_index_prediction.json'\n",
        "    elif exp_type =='COL':\n",
        "      train_pth = 'training_covid-19_data_with_column_label_prediction.json'\n",
        "      test_pth = 'testing_covid-19_data_with_column_label_prediction.json'\n",
        "\n",
        "  if train_pth is None or test_pth is None:\n",
        "    raise Exception(\"Cannot Load dataset\")\n",
        "\n",
        "  with open(os.path.join(base_pth,train_pth),'r',encoding='utf-8') as f:\n",
        "    train_data_json = [json.loads(i) for i in f.readlines()]\n",
        "  with open(os.path.join(base_pth,test_pth),'r',encoding='utf-8') as f:\n",
        "    test_data_json = [json.loads(i) for i in f.readlines()]\n",
        "  return train_data_json,test_data_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8ilMP3ij4Nd"
      },
      "source": [
        "train_data_json,test_data_json = get_data(data_type=USE_CASE,exp_type=EXP_TYPE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uT8WsGUmaU-",
        "outputId": "e4d89e42-21d3-452b-f4cd-7fa837447a06"
      },
      "source": [
        "\n",
        "\n",
        "NUM_TEST = len(test_data_json)\n",
        "NUM_TEST"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JogTVFZBt0Za"
      },
      "source": [
        "dats = []\n",
        "for i in train_data_json:\n",
        "  dats.extend(i['label_index'])\n",
        "NUM_LABELS = len(set(dats))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsYK844ikRzV"
      },
      "source": [
        "fd={i:0 for i in dats}\n",
        "for i in dats:\n",
        "  fd[i]+=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0fDywXCZo2x"
      },
      "source": [
        "fd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4WiBHaYyzQU",
        "outputId": "5c399e67-5529-4acc-cae8-e3fd71a04163"
      },
      "source": [
        "NUM_LABELS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8hWJYMsi6wo"
      },
      "source": [
        "total = sum([fd[f] for f in fd])\n",
        "wt_init = [1/(fd[f]/total) for f in fd]\n",
        "WEIGHTS = [w/sum(wt_init) for w in wt_init]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qea7aT8Wjk5D"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otcVAzyunJjv"
      },
      "source": [
        "from torch.utils.data import (\n",
        "    random_split,\n",
        "    DataLoader,\n",
        "    RandomSampler,\n",
        "    Subset,\n",
        "    TensorDataset\n",
        ")\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    BertTokenizer\n",
        ")\n",
        "from pytorch_lightning import Trainer, seed_everything\n",
        "seed_everything(42)\n",
        "\n",
        "\n",
        "class DocumentDataPreprocessor():\n",
        "    \"\"\"DocumentDataPreprocessor \n",
        "    \"\"\"\n",
        "    CLASS_TOKEN = '[CLS]'\n",
        "    SEP_TOKEN = '[EOS]'\n",
        "    SPECIAL_TOKENS = []\n",
        "\n",
        "    def __init__(self,tokenizer:BertTokenizer,\\\n",
        "                column_split_order=[],\n",
        "                ):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def get_tokenized_text(self,content_text,max_length=1024,pad_to_max_length=True):\n",
        "        attention_mask = []\n",
        "        input_ids = []\n",
        "        encoded_dict = self.tokenizer.encode_plus(\n",
        "                    content_text,                      # Sentence to encode.\n",
        "                    add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                    max_length = max_length,           # Pad & truncate all sentences.\n",
        "                    padding = 'max_length',\n",
        "                    truncation=True,\n",
        "                    return_attention_mask=True,\n",
        "                    return_tensors = 'pt',     # Return pytorch tensors.\n",
        "            )\n",
        "    \n",
        "        # Add the encoded sentence to the list.    \n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_mask.append(encoded_dict['attention_mask'])\n",
        "        # And its attention mask (simply differentiates padding from non-padding).\n",
        "        input_ids = torch.cat(input_ids,dim=0)\n",
        "        attention_mask = torch.cat(attention_mask,dim=0)\n",
        "        return input_ids,attention_mask\n",
        "    \n",
        "\n",
        "    @staticmethod\n",
        "    def split_dataset(dataset,train_percent=0.9):\n",
        "        # Create a split in train-validation \n",
        "        # Calculate the number of samples to include in each set.\n",
        "        if train_percent > 1:\n",
        "            raise Exception('Training Percentage cannot be > 1')\n",
        "        train_size = int(train_percent * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "\n",
        "        # Divide the dataset by randomly selecting samples.\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "        return train_dataset,val_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5G4aOXkz84n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhxi1LfLkS7N"
      },
      "source": [
        "def create_training_set(data_json,tokenizer,exp_type=EXP_TYPE):\n",
        "  import json\n",
        "  proc = DocumentDataPreprocessor(tokenizer)\n",
        "  input_ids,att_mask,labels = [],[],[]\n",
        "  for d in data_json:\n",
        "    json_value = json.dumps(d['value'])\n",
        "    i,m = proc.get_tokenized_text(json_value,max_length=70)\n",
        "    input_ids.append(i)\n",
        "    att_mask.append(m)\n",
        "    if exp_type == 'KEY':\n",
        "        labels.append(d['label_index'][0])\n",
        "    elif exp_type == 'COL':\n",
        "      hot_tensor= F.one_hot(torch.LongTensor([r-1 for r in d['label_index']]),num_classes=NUM_LABELS).sum(dim=0)\n",
        "      labels.append(hot_tensor)\n",
        "\n",
        "  input_ids = torch.cat(input_ids,dim=0)\n",
        "  att_mask = torch.cat(att_mask,dim=0)\n",
        "  if exp_type == 'KEY':\n",
        "    labels = torch.Tensor(labels)\n",
        "  elif exp_type == 'COL':\n",
        "    labels = torch.stack(labels,dim=0)\n",
        "  # print(\"labels\",labels.shape)\n",
        "  return TensorDataset(input_ids,att_mask,labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn5wAgeAmSMS"
      },
      "source": [
        "# json.dumps(train_data_json[0])\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)#prajjwal1/bert-tiny\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-zMQ-yqn_9t"
      },
      "source": [
        "train_dataset = create_training_set(train_data_json,tokenizer)\n",
        "train_dataset,val_dataset = DocumentDataPreprocessor.split_dataset(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDRKSc4Un_7d"
      },
      "source": [
        "test_dataset =  create_training_set(test_data_json,tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-56sA-Hbk79D"
      },
      "source": [
        "train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
        "test_loader = DataLoader(test_dataset,batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccnuEBRZAuSS"
      },
      "source": [
        "val_loader = DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wsaVsQcM1TL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MQRcM0OjpwM"
      },
      "source": [
        "# Model Description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuFymrCNjfqx"
      },
      "source": [
        "from transformers import BertModel,AdamW,get_cosine_with_hard_restarts_schedule_with_warmup,AutoModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSYCoCH6zffm"
      },
      "source": [
        "import torch.nn as nn\n",
        "import einops\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQOdYwww0Fj8"
      },
      "source": [
        "class PredictionHeadTransform(nn.Module):\n",
        "    def __init__(self, hidden_size=768,layer_norm_eps=0.00001):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
        "        self.transform_act_fn = torch.nn.GELU()\n",
        "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class PredictionHead(nn.Module):\n",
        "    def __init__(self, hidden_size=768,layer_norm_eps=0.00001,num_preds=NUM_LABELS):\n",
        "        super().__init__()\n",
        "        # self.transform = PredictionHeadTransform(hidden_size=hidden_size,layer_norm_eps=layer_norm_eps)\n",
        "        # The output weights are the same as the input embeddings, but there is\n",
        "        # an output-only bias for each token.\n",
        "        self.decoder = nn.Linear(hidden_size, num_preds)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # hidden_states = self.transform(hidden_states)\n",
        "        hidden_states = self.decoder(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "class BertDataIntegrationClassifier(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    self.pred_head = PredictionHead(hidden_size=self.model.config.hidden_size,layer_norm_eps=self.model.config.layer_norm_eps)\n",
        "    self.sfmx = nn.Softmax(dim=1)\n",
        "  \n",
        "  def forward(self,input,att_mask=None):\n",
        "    dx = self.model(input,attention_mask=att_mask)\n",
        "    val = dx.pooler_output\n",
        "    final_st = dx.pooler_output # val[:,0] # Getting classtoken op\n",
        "    return self.pred_head(final_st) #self.sfmx()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jvbZ3254d9L"
      },
      "source": [
        "# model = BertDataIntegrationClassifier()\n",
        "import pytorch_lightning as pl\n",
        "import pytorch_lightning.metrics.functional.accuracy as get_accuracy\n",
        "from pytorch_lightning.metrics.functional import f1 as f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE_FDGKrPyuW"
      },
      "source": [
        "### Vanilla Self Attention\n",
        "- NO bs of Other things like mask/cross attention etc.  \n",
        "- Attention defined by\n",
        "  - $$ W_Q,W_K,W_V: \\text{Are key,query,and value matrixes}$$\n",
        "  - $$attention = (softmax(W_Q \\times W_K)*scale) \\times W_V$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX1p3q-E_WbM"
      },
      "source": [
        "class SimpleSelfAttention(nn.Module):\n",
        "  '''\n",
        "  Vanilla Self attention on sequence. No Masking etc.\n",
        "  '''\n",
        "  def __init__(self,hidden_size,dropout=0.1,num_heads=4,scale=0.2,mlp_dim=3072):\n",
        "    super().__init__()\n",
        "    self.kqv_layer = nn.Linear(hidden_size,3*hidden_size)\n",
        "    self.num_heads = num_heads\n",
        "    self.ff_layer = nn.Sequential(\n",
        "          nn.Linear(hidden_size, hidden_size),\n",
        "          nn.Dropout(dropout)\n",
        "    )\n",
        "    \n",
        "    self.scale = hidden_size ** -scale\n",
        "\n",
        "  def forward(self,sequence_embedding):\n",
        "    # print(f\"Shape of Sequence Embedding {sequence_embedding.shape}\")\n",
        "    kqv = self.kqv_layer(sequence_embedding).chunk(3, dim = -1)\n",
        "    # print(f\"Shape of kqv {kqv[0].shape,kqv[1].shape}\")\n",
        "    k,q,v = map(lambda x:einops.rearrange(x,'b s (h d) -> b h s d',h=self.num_heads),kqv)\n",
        "    scaled_dot_product = torch.einsum('bhsd,bhnd->bhsn',k,q) * self.scale\n",
        "    weighted_sum = F.softmax(scaled_dot_product,dim=-1)\n",
        "    value_weighted_sum = torch.einsum('bhsn,bhsd->bhnd',weighted_sum,v)\n",
        "    reweighted_sequence_embedding = einops.rearrange(value_weighted_sum,'b h s d -> b s (h d)',h=self.num_heads)\n",
        "    return self.ff_layer(reweighted_sequence_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH1Z0Yxo_Ya5"
      },
      "source": [
        "import math\n",
        "class Conv1D(nn.Module):\n",
        "    \"\"\"\n",
        "    1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).\n",
        "    Basically works like a linear layer but the weights are transposed.\n",
        "    Args:\n",
        "        nf (:obj:`int`): The number of output features.\n",
        "        nx (:obj:`int`): The number of input features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nf, nx):\n",
        "        super().__init__()\n",
        "        self.nf = nf\n",
        "        w = torch.empty(nx, nf)\n",
        "        nn.init.normal_(w, std=0.02)\n",
        "        self.weight = nn.Parameter(w)\n",
        "        self.bias = nn.Parameter(torch.zeros(nf))\n",
        "\n",
        "    def forward(self, x):\n",
        "        size_out = x.size()[:-1] + (self.nf,)\n",
        "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
        "        x = x.view(*size_out)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bVe-5hKhs2w"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_state,embedding_size=256,resid_pdrop=0.1):  # in MLP: n_state=(n * embedding_size)\n",
        "        super().__init__()\n",
        "        nx = embedding_size # n_state = outputfeatures\n",
        "        self.c_fc = Conv1D(n_state, nx)\n",
        "        self.c_proj = Conv1D(nx, n_state) # nx = inputfeatures\n",
        "        self.act = nn.GELU()\n",
        "        self.dropout = nn.Dropout(resid_pdrop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.act(self.c_fc(x))\n",
        "        h2 = self.c_proj(h)\n",
        "        return self.dropout(h2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xL50OaNAjEW"
      },
      "source": [
        "##  https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit_pytorch.py\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OenWOp8Ikte"
      },
      "source": [
        "## Code Refactored From https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_gpt2.py \n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, \n",
        "                 embedding_size=256,\\\n",
        "                 layer_norm_epsilon=0.00001,\\\n",
        "                 scale=0.2,\\\n",
        "                 resid_pdrop=0.1,\\\n",
        "                 attn_pdrop=0.1,\\\n",
        "                 num_attention_heads = 8):\n",
        "        super().__init__()\n",
        "        hidden_size = embedding_size\n",
        "        inner_dim = 4 * hidden_size\n",
        "        self.ln_1 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
        "        self.attn = Residual(SimpleSelfAttention(hidden_size,num_heads = num_attention_heads,scale=scale,dropout=attn_pdrop,mlp_dim=inner_dim))\n",
        "        self.ln_2 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
        "        self.mlp = Residual(MLP(inner_dim,embedding_size=embedding_size,resid_pdrop=resid_pdrop))\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "    ):\n",
        "        attn_outputs = self.attn(\n",
        "            self.ln_1(hidden_states),\n",
        "        )\n",
        "        feed_forward_hidden_states = self.mlp(self.ln_2(attn_outputs))\n",
        "        return feed_forward_hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH3l1oVBBQKh"
      },
      "source": [
        "### Cross-Modal-Attention\n",
        "\n",
        "- Inspiration From https://arxiv.org/abs/1906.00295\n",
        "\n",
        "$$ Y_\\alpha = CM_{\\beta\\to\\alpha}(X_\\alpha,X_\\beta)$$\n",
        "$$ Y_\\alpha = softmax(\\frac{Q_\\alpha \\times K^T_\\beta}{\\sqrt{d_k}})V_\\beta$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l7LO0xszDJr"
      },
      "source": [
        "class CrossModalAttention(nn.Module):\n",
        "  def __init__(self,hidden_size,dropout=0.1,num_heads=4,scale=0.2):\n",
        "    super().__init__()\n",
        "    self.kv_layer = nn.Linear(hidden_size,2*hidden_size)\n",
        "    self.q_layer = nn.Linear(hidden_size,hidden_size)\n",
        "    self.num_heads = num_heads\n",
        "    self.ff_layer = nn.Sequential(\n",
        "          nn.Linear(hidden_size, hidden_size),\n",
        "          nn.Dropout(dropout)\n",
        "    )\n",
        "    self.scale = hidden_size**-scale\n",
        "\n",
        "  def forward(self,seq_x,seq_y,mask=True):\n",
        "    '''\n",
        "    r(seq_x|seq_y)\n",
        "    r(action|text)\n",
        "    r(text|action)\n",
        "      action: b t d \n",
        "          - b: batchsize\n",
        "          - t: trajectory length \n",
        "          - d : hidden dims\n",
        "      text : b s d\n",
        "          - b: batchsize\n",
        "          - s : length of text\n",
        "          - d: hidden dims\n",
        "      *d will be same in text and traj\n",
        "    '''\n",
        "    kv = self.kv_layer(seq_y).chunk(2, dim = -1)\n",
        "    q = einops.rearrange(self.q_layer(seq_x),'b s (h d) -> b h s d',h=self.num_heads)\n",
        "    k,v = map(lambda x:einops.rearrange(x,'b s (h d) -> b h s d',h=self.num_heads),kv)\n",
        "    scaled_dot_product = torch.einsum('bhsd,bhnd->bhsn',k,q) * self.scale\n",
        "    # add mask here for better performance. gi\n",
        "\n",
        "    weighted_sum = F.softmax(scaled_dot_product,dim=-1)\n",
        "    value_weighted_sum = torch.einsum('bhsn,bhsd->bhnd',weighted_sum,v)\n",
        "    reweighted_sequence_embedding = einops.rearrange(value_weighted_sum,'b h s d -> b s (h d)',h=self.num_heads)\n",
        "    return self.ff_layer(reweighted_sequence_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drs5GNXl-p2L"
      },
      "source": [
        "class CrossModalAttentionWithMask(nn.Module):\n",
        "  def __init__(self,hidden_size,dropout=0.1,num_heads=4,scale=0.2):\n",
        "    super().__init__()\n",
        "    self.kv_layer = nn.Linear(hidden_size,2*hidden_size)\n",
        "    self.q_layer = nn.Linear(hidden_size,hidden_size)\n",
        "    self.num_heads = num_heads\n",
        "    self.ff_layer = nn.Sequential(\n",
        "          nn.Linear(hidden_size, hidden_size),\n",
        "          nn.Dropout(dropout)\n",
        "    )\n",
        "    self.scale = hidden_size**-scale\n",
        "\n",
        "  def forward(self,seq_x,seq_y,mask=None):\n",
        "    '''\n",
        "    r(seq_x|seq_y)\n",
        "    r(action|text)\n",
        "    r(text|action)\n",
        "      action: b t d \n",
        "          - b: batchsize\n",
        "          - t: trajectory length \n",
        "          - d : hidden dims\n",
        "      text : b s d\n",
        "          - b: batchsize\n",
        "          - s : length of text\n",
        "          - d: hidden dims\n",
        "      *d will be same in text and traj\n",
        "    '''\n",
        "    kv = self.kv_layer(seq_y).chunk(2, dim = -1)\n",
        "    q = einops.rearrange(self.q_layer(seq_x),'b s (h d) -> b h s d',h=self.num_heads)\n",
        "    k,v = map(lambda x:einops.rearrange(x,'b s (h d) -> b h s d',h=self.num_heads),kv)\n",
        "    scaled_dot_product = torch.einsum('bhsd,bhnd->bhsn',k,q) * self.scale\n",
        "    # add mask here for better performance. gi\n",
        "    if mask is not None:\n",
        "      # print(f\"Shape Of Mask {mask.shape}\")\n",
        "      mask_vals = self.get_extended_attention_mask(mask,seq_x.size(),device=seq_x.device)\n",
        "      scaled_dot_product+=mask_vals\n",
        "    # return scaled_dot_product\n",
        "    weighted_sum = F.softmax(scaled_dot_product,dim=-1)\n",
        "    value_weighted_sum = torch.einsum('bhsn,bhsd->bhnd',weighted_sum,v)\n",
        "    reweighted_sequence_embedding = einops.rearrange(value_weighted_sum,'b h s d -> b s (h d)',h=self.num_heads)\n",
        "    return self.ff_layer(reweighted_sequence_embedding)\n",
        "\n",
        "  @staticmethod\n",
        "  def get_extended_attention_mask(attention_mask, input_shape, device=torch.device('cpu'),is_decoder=False):\n",
        "      \"\"\" Thank You Hugging Face : \n",
        "      https://github.com/huggingface/transformers/blob/443f67e887a030d8254eba126e5f2cdb8b70eb63/src/transformers/modeling_utils.py\n",
        "      Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
        "      Arguments:\n",
        "          attention_mask (:obj:`torch.Tensor`):\n",
        "              Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
        "          input_shape (:obj:`Tuple[int]`):\n",
        "              The shape of the input to the model.\n",
        "          device: (:obj:`torch.device`):\n",
        "              The device of the input to the model.\n",
        "      Returns:\n",
        "          :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n",
        "      \"\"\"\n",
        "      # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "      # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "      if attention_mask.dim() == 3:\n",
        "          extended_attention_mask = attention_mask[:, None, :, :]\n",
        "      elif attention_mask.dim() == 2:\n",
        "          # Provided a padding mask of dimensions [batch_size, seq_length, d]\n",
        "          # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
        "          # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "          if is_decoder:\n",
        "              batch_size, seq_length, d = input_shape\n",
        "              seq_ids = torch.arange(seq_length, device=device)\n",
        "              causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n",
        "              # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n",
        "              # causal and attention masks must have same type with pytorch version < 1.3\n",
        "              causal_mask = causal_mask.to(attention_mask.dtype)\n",
        "\n",
        "              if causal_mask.shape[1] < attention_mask.shape[1]:\n",
        "                  prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n",
        "                  causal_mask = torch.cat(\n",
        "                      [\n",
        "                          torch.ones(\n",
        "                              (batch_size, seq_length, prefix_seq_len), device=device, dtype=causal_mask.dtype\n",
        "                          ),\n",
        "                          causal_mask,\n",
        "                      ],\n",
        "                      axis=-1,\n",
        "                  )\n",
        "\n",
        "              extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n",
        "          else:\n",
        "              extended_attention_mask = attention_mask[:, None, None, :]\n",
        "      else:\n",
        "          raise ValueError(\n",
        "              \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n",
        "                  input_shape, attention_mask.shape\n",
        "              )\n",
        "          )\n",
        "\n",
        "      # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "      # masked positions, this operation will create a tensor which is 0.0 for\n",
        "      # positions we want to attend and -10000.0 for masked positions.\n",
        "      # Since we are adding it to the raw scores before the softmax, this is\n",
        "      # effectively the same as removing these entirely.\n",
        "      # extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
        "      extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "      return extended_attention_mask\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa4PT-SvHF1f"
      },
      "source": [
        "class MultiModalAttentionBlock(nn.Module):\n",
        "    def __init__(self, \n",
        "                 embedding_size=256,\\\n",
        "                 layer_norm_epsilon=0.00001,\\\n",
        "                 scale=False,\\\n",
        "                 resid_pdrop=0.1,\\\n",
        "                 attn_pdrop=0.1,\\\n",
        "                 num_attention_heads = 8):\n",
        "        super().__init__()\n",
        "        hidden_size = embedding_size\n",
        "        inner_dim = 4 * hidden_size\n",
        "        self.ln_1 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
        "        self.attn = CrossModalAttentionWithMask(hidden_size,num_heads = num_attention_heads,scale=scale,dropout=attn_pdrop)\n",
        "        self.ln_2 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
        "        self.mlp = Residual(MLP(inner_dim,embedding_size=embedding_size,resid_pdrop=resid_pdrop))\n",
        "    def forward(\n",
        "        self,\n",
        "        seq_x,seq_y,mask=None\n",
        "    ):\n",
        "        attn_outputs = self.attn(\n",
        "            self.ln_1(seq_x),self.ln_1(seq_y),mask=mask\n",
        "        )\n",
        "        # Residual connection\n",
        "        attn_outputs = seq_x + attn_outputs\n",
        "        feed_forward_hidden_states = self.mlp(self.ln_2(attn_outputs))\n",
        "        return feed_forward_hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2s7AbDsIQXp"
      },
      "source": [
        "#  https://github.com/yaohungt/Multimodal-Transformer/blob/master/modules/position_embedding.py\n",
        "# Code adapted from the fairseq repo.\n",
        "import math\n",
        "def make_positions(tensor, padding_idx, left_pad):\n",
        "    \"\"\"Replace non-padding symbols with their position numbers.\n",
        "    Position numbers begin at padding_idx+1.\n",
        "    Padding symbols are ignored, but it is necessary to specify whether padding\n",
        "    is added on the left side (left_pad=True) or right side (left_pad=False).\n",
        "    \"\"\"\n",
        "    max_pos = padding_idx + 1 + tensor.size(1)\n",
        "    device = tensor.get_device()\n",
        "    buf_name = f'range_buf_{device}'\n",
        "    if not hasattr(make_positions, buf_name):\n",
        "        setattr(make_positions, buf_name, tensor.new())\n",
        "    setattr(make_positions, buf_name, getattr(make_positions, buf_name).type_as(tensor))\n",
        "    if getattr(make_positions, buf_name).numel() < max_pos:\n",
        "        torch.arange(padding_idx + 1, max_pos, out=getattr(make_positions, buf_name))\n",
        "    mask = tensor.ne(padding_idx)\n",
        "    positions = getattr(make_positions, buf_name)[:tensor.size(1)].expand_as(tensor)\n",
        "    if left_pad:\n",
        "        positions = positions - mask.size(1) + mask.long().sum(dim=1).unsqueeze(1)\n",
        "    new_tensor = tensor.clone()\n",
        "    return new_tensor.masked_scatter_(mask, positions[mask]).long()\n",
        "\n",
        "\n",
        "class SinusoidalPositionalEmbedding(nn.Module):\n",
        "    \"\"\"This module produces sinusoidal positional embeddings of any length.\n",
        "    Padding symbols are ignored, but it is necessary to specify whether padding\n",
        "    is added on the left side (left_pad=True) or right side (left_pad=False).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim, padding_idx=0, left_pad=0, init_size=128):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.padding_idx = padding_idx\n",
        "        self.left_pad = left_pad\n",
        "        self.weights = dict()   # device --> actual weight; due to nn.DataParallel :-(\n",
        "        self.register_buffer('_float_tensor', torch.FloatTensor(1))\n",
        "\n",
        "    @staticmethod\n",
        "    def get_embedding(num_embeddings, embedding_dim, padding_idx=None):\n",
        "        \"\"\"Build sinusoidal embeddings.\n",
        "        This matches the implementation in tensor2tensor, but differs slightly\n",
        "        from the description in Section 3.5 of \"Attention Is All You Need\".\n",
        "        \"\"\"\n",
        "        half_dim = embedding_dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)\n",
        "        emb = torch.arange(num_embeddings, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)\n",
        "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(num_embeddings, -1)\n",
        "        if embedding_dim % 2 == 1:\n",
        "            # zero pad\n",
        "            emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)\n",
        "        if padding_idx is not None:\n",
        "            emb[padding_idx, :] = 0\n",
        "        return emb\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
        "        bsz, seq_len = input.size()\n",
        "        max_pos = self.padding_idx + 1 + seq_len\n",
        "        device = input.get_device()\n",
        "        if device not in self.weights or max_pos > self.weights[device].size(0):\n",
        "            # recompute/expand embeddings if needed\n",
        "            self.weights[device] = SinusoidalPositionalEmbedding.get_embedding(\n",
        "                max_pos,\n",
        "                self.embedding_dim,\n",
        "                self.padding_idx,\n",
        "            )\n",
        "        self.weights[device] = self.weights[device].type_as(self._float_tensor)\n",
        "        positions = make_positions(input, self.padding_idx, self.left_pad)\n",
        "        return self.weights[device].index_select(0, positions.view(-1)).view(bsz, seq_len, -1).detach()\n",
        "\n",
        "    def max_positions(self):\n",
        "        \"\"\"Maximum number of supported positions.\"\"\"\n",
        "        return int(1e5)  # an arbitrary large number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpwoxTUEHM17"
      },
      "source": [
        "class MultiModalTransformer(nn.Module):\n",
        "  def __init__(self,\n",
        "              num_layers = 4,\n",
        "              embed_dropout =0.1 ,\n",
        "              embedding_size=256,\\\n",
        "              layer_norm_epsilon=0.00001,\\\n",
        "              scale=0.2,\\\n",
        "              resid_pdrop=0.1,\\\n",
        "              attn_pdrop=0.1,\\\n",
        "              num_attention_heads = 8):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([])\n",
        "    self.embed_dropout=embed_dropout\n",
        "    self.embed_scale = math.sqrt(embedding_size)\n",
        "    self.embed_positions = SinusoidalPositionalEmbedding(embedding_size)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "      \n",
        "        self.layers.append(MultiModalAttentionBlock(\n",
        "                          embedding_size=embedding_size,\\\n",
        "                          layer_norm_epsilon=layer_norm_epsilon,\\\n",
        "                          scale=scale,\\\n",
        "                          resid_pdrop=resid_pdrop,\\\n",
        "                          attn_pdrop=attn_pdrop,\\\n",
        "                          num_attention_heads = num_attention_heads))\n",
        "\n",
        "  def forward(self, seq_x,seq_y,mask=None):\n",
        "    # Add positional embedding\n",
        "    seq_x = self.embed_scale * seq_x\n",
        "    if self.embed_positions is not None:\n",
        "        seq_x += self.embed_positions(seq_x[:, :, 0])   \n",
        "    seq_x = F.dropout(seq_x, p=self.embed_dropout, training=self.training)\n",
        "\n",
        "    seq_y = self.embed_scale * seq_y\n",
        "    if self.embed_positions is not None:\n",
        "        seq_y += self.embed_positions(seq_y[:, :, 0])   \n",
        "    seq_y = F.dropout(seq_y, p=self.embed_dropout, training=self.training)\n",
        "    \n",
        "    hidden = seq_x\n",
        "    for attention_block in self.layers:\n",
        "        hidden = attention_block(hidden,seq_y,mask=mask)\n",
        "    return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am2p5ifknJVu"
      },
      "source": [
        "class VanillaTransformer(nn.Module):\n",
        "  '''\n",
        "  Contains Sinusoidal Embedding for sequences as part of the Framework. \n",
        "  '''\n",
        "  def __init__(self,\n",
        "              num_layers = 4,\n",
        "              embed_dropout =0.1 ,\\\n",
        "              embedding_size=256,\\\n",
        "              layer_norm_epsilon=0.00001,\\\n",
        "              scale=0.2,\\\n",
        "              resid_pdrop=0.1,\\\n",
        "              attn_pdrop=0.1,\\\n",
        "              num_attention_heads = 8):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([])\n",
        "    self.embed_dropout=embed_dropout\n",
        "    self.embed_scale = math.sqrt(embedding_size)\n",
        "    self.embed_positions = SinusoidalPositionalEmbedding(embedding_size)\n",
        "\n",
        "    for _ in range(num_layers): \n",
        "        self.layers.append(Block(\n",
        "                          embedding_size=embedding_size,\\\n",
        "                          layer_norm_epsilon=layer_norm_epsilon,\\\n",
        "                          scale=scale,\\\n",
        "                          resid_pdrop=resid_pdrop,\\\n",
        "                          attn_pdrop=attn_pdrop,\\\n",
        "                          num_attention_heads = num_attention_heads))\n",
        "\n",
        "  def forward(self, x, mask = None):\n",
        "    # Add positional embedding\n",
        "    x = self.embed_scale * x # (b,len,d)\n",
        "    if self.embed_positions is not None:\n",
        "        x += self.embed_positions(x[:, :, 0])   \n",
        "    x = F.dropout(x, p=self.embed_dropout, training=self.training)\n",
        "\n",
        "    hidden = x\n",
        "    for attention_block in self.layers:\n",
        "        hidden = attention_block(hidden)\n",
        "    return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN-qQp1njvSV"
      },
      "source": [
        "## Ground Up Transformer with Bert Tokenized Embedding. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETtNdkhG8Rka"
      },
      "source": [
        "## Transformer model based on pretrained BERT "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dquqL1B7PNOb"
      },
      "source": [
        "class BertEmbedTransformer(nn.Module):\n",
        "  def __init__(self,\n",
        "              num_layers=8,\\\n",
        "              embedding_size=128,\\\n",
        "              layer_norm_epsilon=0.00001,\\\n",
        "              scale=0.01,\\\n",
        "              resid_pdrop=0.1,\\\n",
        "              attn_pdrop=0.1,\\\n",
        "              num_attention_heads = 8,\\\n",
        "              embd_pdrop=0.1,\n",
        "              num_actions=3,\\\n",
        "              common_conv_dim=64):\n",
        "    super().__init__()\n",
        "    data_input = dict(\n",
        "        embedding_size=embedding_size,\n",
        "        num_layers =num_layers,\n",
        "        layer_norm_epsilon =layer_norm_epsilon,\n",
        "        scale =scale,\n",
        "        resid_pdrop =resid_pdrop,\n",
        "        attn_pdrop =attn_pdrop,\n",
        "        num_attention_heads =num_attention_heads,\n",
        "    )\n",
        "    self.transformer = VanillaTransformer(**data_input)\n",
        "    bert_model = AutoModel.from_pretrained(PRETRAINED_MODEL)\n",
        "    bert_emb = bert_model.embeddings.word_embeddings\n",
        "    text_embedding_dim = bert_emb.embedding_dim\n",
        "    num_emb = bert_emb.num_embeddings\n",
        "    self.text_embeddings = nn.Embedding(num_embeddings=num_emb,embedding_dim=text_embedding_dim)\n",
        "    # bert_model.embeddings.word_embeddings\n",
        "    self.text_embeddings.load_state_dict(bert_emb.state_dict())\n",
        "    # self.text_embeddings,_ , text_embedding_dim = self.create_emb_layer(glove_weights)\n",
        "    self.text_embeddings.weight.requires_grad = False\n",
        "    self.text_conv = nn.Conv1d(text_embedding_dim,common_conv_dim,kernel_size=1,padding=0,bias=False)\n",
        "    self.text_cls_token = nn.Parameter(torch.randn(1, 1, common_conv_dim))\n",
        "    self.to_cls = nn.Identity()\n",
        "    self.final_layer = nn.Sequential(\n",
        "        nn.Linear(common_conv_dim, common_conv_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(embd_pdrop),\n",
        "        nn.Linear(common_conv_dim, common_conv_dim)\n",
        "    )\n",
        "    \n",
        "  def forward(self,inputs,att_mask=None):\n",
        "    text_tensor = self.text_embeddings(inputs)\n",
        "    \n",
        "    text_tensor = text_tensor.transpose(1,2)\n",
        "\n",
        "    text_tensor = self.text_conv(text_tensor).permute(0,2,1)\n",
        "    \n",
        "    # Prepend CLS tokens and Finally extract thoose instead of the last token.\n",
        "    b,n,_ = text_tensor.size()\n",
        "    text_cls_token = einops.repeat(self.text_cls_token,'() n d -> b n d', b = b)\n",
        "\n",
        "    text_tensor = torch.cat((text_cls_token,text_tensor),dim=1)\n",
        "    # adding Extra one for cls tokens that get prepended the tensors\n",
        "    if att_mask!=None:\n",
        "      att_mask = torch.cat((torch.ones(b).unsqueeze(1).to(text_tensor.device),att_mask),dim=1)\n",
        "    \n",
        "    text_j_tensor = self.transformer(text_tensor,mask=att_mask)\n",
        "    \n",
        "    l_txt = self.to_cls(text_j_tensor[:,0])\n",
        "    \n",
        "    # A residual block\n",
        "    concat_tensor_proj = self.final_layer(l_txt)\n",
        "    concat_tensor_proj+=l_txt\n",
        "    \n",
        "    return concat_tensor_proj \n",
        "    \n",
        "\n",
        "class TransformerDIClassifier(torch.nn.Module):\n",
        "  def __init__(self,\n",
        "              num_layers=8,\\\n",
        "              embedding_size=64,\\\n",
        "              layer_norm_epsilon=0.00001,\\\n",
        "              scale=0.01,\\\n",
        "              resid_pdrop=0.1,\\\n",
        "              attn_pdrop=0.1,\\\n",
        "              num_attention_heads = 8,\\\n",
        "              embd_pdrop=0.1,\n",
        "              num_actions=3,\\\n",
        "              common_conv_dim=64):\n",
        "    super().__init__()\n",
        "    self.model = BertEmbedTransformer(\n",
        "        num_layers = num_layers,\n",
        "        embedding_size = embedding_size,\n",
        "        layer_norm_epsilon = layer_norm_epsilon,\n",
        "        scale = scale,\n",
        "        resid_pdrop = resid_pdrop,\n",
        "        attn_pdrop = attn_pdrop,\n",
        "        num_attention_heads = num_attention_heads,\n",
        "        embd_pdrop = embd_pdrop,\n",
        "        num_actions = num_actions,\n",
        "        common_conv_dim = common_conv_dim,\n",
        "    )\n",
        "    self.pred_head = PredictionHead(hidden_size=common_conv_dim,layer_norm_eps=layer_norm_epsilon)\n",
        "    \n",
        "  \n",
        "  def forward(self,input,att_mask=None):\n",
        "    final_st = self.model(input,att_mask=att_mask)\n",
        "    return self.pred_head(final_st)  #self.sfmx()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30c3eibUaxip"
      },
      "source": [
        "## Lightning Module\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuJkC-oT8ohi"
      },
      "source": [
        "## module for doing singel label prediction (e.g key index prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2MbT80a4j97"
      },
      "source": [
        "logit_list = []\n",
        "label_list = []\n",
        "class DataIntegrationClassifier(pl.LightningModule):\n",
        "  def __init__(self,with_mask=False,is_bert=IS_BERT):\n",
        "    super().__init__()\n",
        "    # self.model =  TransformerDIClassifier()\n",
        "    self.with_mask = with_mask\n",
        "    if is_bert:\n",
        "      print(\"Using BERT Model\")\n",
        "      self.model = BertDataIntegrationClassifier()\n",
        "    else:\n",
        "      print(\"Using Vanilla Backbone Model\")\n",
        "      self.model = TransformerDIClassifier(**CORE_TRANSFORMER_PARAMS)\n",
        "    # self.cross_entropy = nn.CrossEntropyLoss(torch.Tensor(WEIGHTS))\n",
        "    self.cross_entropy = nn.CrossEntropyLoss()\n",
        "    self.sfmx = nn.Softmax(dim=1)\n",
        "    \n",
        "  def forward(self,inputs,att_mask=None): # state,action should be full trajectory sequences for state and action for each element in the batch.  \n",
        "    '''\n",
        "    inputs : (input_ids)\n",
        "        - input_ids: b k :\n",
        "          - b: batchsize\n",
        "          - k: sequence_length\n",
        "    *_mask = mask: b s : binary tensor. \n",
        "    '''\n",
        "    return self.model(inputs,att_mask=att_mask)\n",
        "\n",
        "\n",
        "  def training_step(self,batch,batch_nb):\n",
        "    inps,mask,labels = batch\n",
        "    labels=(labels-1).type(torch.LongTensor).to(self.device)\n",
        "    if not self.with_mask:\n",
        "      logits = self.sfmx(self(inps,att_mask=None))\n",
        "    else:\n",
        "      logits = self.sfmx(self(inps,att_mask=mask))\n",
        "    loss = self.cross_entropy(logits,labels)\n",
        "    accuracy = get_accuracy(logits,labels)\n",
        "    f1 = f1_score(\n",
        "        torch.argmax(logits.detach(),dim=1),labels,num_classes=NUM_LABELS\n",
        "    )\n",
        "    self.logger.log_metrics({\n",
        "        'train_f1' : f1.detach().cpu().numpy(),\n",
        "        'train_accuracy':accuracy.detach().cpu().numpy(),\n",
        "        # 'train_loss_embed':(loss_embed_pc+loss_embed_nc).detach().cpu().numpy(),\n",
        "        'train_loss':loss.detach().cpu().numpy(),\n",
        "        'epoch': self.current_epoch,\n",
        "    })\n",
        "    return {'loss':loss}\n",
        "\n",
        "\n",
        "  def validation_step(self,batch,batch_nb):\n",
        "    inps,mask,labels = batch\n",
        "    labels=(labels-1).type(torch.LongTensor).to(self.device)\n",
        "    if not self.with_mask:\n",
        "      logits = self.sfmx(self(inps,att_mask=None))\n",
        "    else:\n",
        "      logits = self.sfmx(self(inps,att_mask=mask))\n",
        "    loss = self.cross_entropy(logits,labels)\n",
        "    accuracy = get_accuracy(logits,labels)\n",
        "    f1 = f1_score(\n",
        "        torch.argmax(logits.detach(),dim=1),labels,num_classes=NUM_LABELS\n",
        "    )\n",
        "    self.logger.log_metrics({\n",
        "        'val_f1' : f1.detach().cpu().numpy(),\n",
        "        'val_accuracy':accuracy.detach().cpu().numpy(),\n",
        "        'val_loss':loss.detach().cpu().numpy(),\n",
        "        'epoch': self.current_epoch,\n",
        "    })\n",
        "    return {'loss':loss,'val_loss':loss}\n",
        "  \n",
        "  def test_step(self,batch,batch_nb):\n",
        "    inps,mask,labels = batch\n",
        "    labels=(labels-1).type(torch.LongTensor).to(self.device)\n",
        "    if not self.with_mask:\n",
        "      logits = self.sfmx(self(inps,att_mask=None))\n",
        "    else:\n",
        "      logits = self.sfmx(self(inps,att_mask=mask))\n",
        "    loss = self.cross_entropy(logits,labels)\n",
        "    with open('logits.txt','w') as w:\n",
        "      for i in logit_list:\n",
        "        w.write(i)\n",
        "       \n",
        "    with open('label.txt','w') as wf:\n",
        "      for i in label_list:\n",
        "        wf.write(i)\n",
        "        wf.write('\\n')\n",
        "    accuracy = get_accuracy(logits,labels)\n",
        "    f1 = f1_score(\n",
        "        torch.argmax(logits.detach(),dim=1),labels,num_classes=NUM_LABELS\n",
        "    )\n",
        "    self.logger.log_metrics({\n",
        "        'test_f1' : f1.detach().cpu().numpy(),\n",
        "        'test_accuracy':accuracy.detach().cpu().numpy(),\n",
        "        'test_loss':loss.detach().cpu().numpy(),\n",
        "        'epoch': self.current_epoch,\n",
        "    })\n",
        "    return {'loss':loss,'test_accuracy':accuracy}\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer =  AdamW(self.parameters(), lr=LEARNING_RATE, eps=1e-12, betas=(0.9,0.999))\n",
        "    num_minibatch_steps = NUM_TRAIN_SAMPLES/BATCH_SIZE\n",
        "    max_epochs = MAX_EPOCHS\n",
        "    warmup = WARMUP\n",
        "    t_total = max_epochs * num_minibatch_steps\n",
        "    num_cycles = MAX_CYCLES\n",
        "    lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, warmup, t_total,num_cycles=num_cycles)\n",
        "    return [optimizer] ,[lr_scheduler]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iBTJGY34gtR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QNZU29g8xfH"
      },
      "source": [
        "## module for doing multi-label predicition (e.g column prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVbr2PG7PKhD"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class DataIntegrationColumnClassifier(pl.LightningModule):\n",
        "  def __init__(self,with_mask=False,is_bert=IS_BERT):\n",
        "    super().__init__()\n",
        "    # self.model =  TransformerDIClassifier()\n",
        "    self.with_mask = with_mask\n",
        "    if is_bert:\n",
        "      print(\"Using BERT Model\")\n",
        "      self.model = BertDataIntegrationClassifier()\n",
        "    else:\n",
        "      print(\"Using Vanilla Backbone Model\")\n",
        "      self.model = TransformerDIClassifier(**CORE_TRANSFORMER_PARAMS)\n",
        "    # self.cross_entropy = nn.CrossEntropyLoss(torch.Tensor(WEIGHTS))\n",
        "    \n",
        "    self.sfmx = nn.Softmax(dim=1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.loss = nn.BCEWithLogitsLoss()\n",
        "    self.sigmoid_threshold = SIGMOID_THRESHOLD\n",
        "    \n",
        "  def forward(self,inputs,att_mask=None): # state,action should be full trajectory sequences for state and action for each element in the batch.  \n",
        "    '''\n",
        "    inputs : (input_ids)\n",
        "        - input_ids: b k :\n",
        "          - b: batchsize\n",
        "          - k: sequence_length\n",
        "    *_mask = mask: b s : binary tensor. \n",
        "    '''\n",
        "    return self.model(inputs,att_mask=att_mask)\n",
        "\n",
        "  def get_multi_label_exact_match_accuracy(self,y_pred,y_true):\n",
        "    pred_vals = (self.sigmoid(y_pred) > self.sigmoid_threshold).long()\n",
        "    num_vals = pred_vals.size()[0]\n",
        "    correct_vals = 0 # Exactly Correct Vals\n",
        "    for v1,v2 in zip(pred_vals,y_true):\n",
        "      if str(v1.cpu().numpy()) == str(v2.cpu().numpy()):\n",
        "        correct_vals+=1\n",
        "    return torch.tensor(correct_vals/num_vals)\n",
        "\n",
        "  \n",
        "\n",
        "  def training_step(self,batch,batch_nb):\n",
        "    inps,mask,labels = batch\n",
        "    # labels=(labels).type(torch.LongTensor).to(self.device)\n",
        "    if not self.with_mask:\n",
        "      logits = self(inps,att_mask=None)\n",
        "    else:\n",
        "      logits = self(inps,att_mask=mask)\n",
        "    loss = self.loss(logits,labels.type_as(logits))\n",
        "    accuracy = self.get_multi_label_exact_match_accuracy(logits,labels)\n",
        "    self.logger.log_metrics({\n",
        "        'train_accuracy':accuracy.detach().cpu().numpy(),\n",
        "        # 'train_loss_embed':(loss_embed_pc+loss_embed_nc).detach().cpu().numpy(),\n",
        "        'train_loss':loss.detach().cpu().numpy(),\n",
        "        'epoch': self.current_epoch,\n",
        "    })\n",
        "    return {'loss':loss}\n",
        "\n",
        "\n",
        "  def validation_step(self,batch,batch_nb):\n",
        "    inps,mask,labels = batch\n",
        "    # labels=(labels).type(torch.LongTensor).to(self.device)\n",
        "    if not self.with_mask:\n",
        "      logits = self(inps,att_mask=None)\n",
        "    else:\n",
        "      logits = self(inps,att_mask=mask)\n",
        "    \n",
        "    loss = self.loss(logits,labels.type_as(logits))\n",
        "    accuracy = self.get_multi_label_exact_match_accuracy(logits,labels)\n",
        "    self.logger.log_metrics({\n",
        "        'val_accuracy':accuracy.detach().cpu().numpy(),\n",
        "        'val_loss':loss.detach().cpu().numpy(),\n",
        "        'epoch': self.current_epoch,\n",
        "    })\n",
        "    return {'loss':loss,'val_loss':loss}\n",
        "  \n",
        "  def test_step(self,batch,batch_nb):\n",
        "    inps,mask,labels = batch\n",
        "    # labels=(labels).type(torch.LongTensor).to(self.device)\n",
        "    if not self.with_mask:\n",
        "      logits = self(inps,att_mask=None)\n",
        "    else:\n",
        "      logits = self(inps,att_mask=mask)\n",
        "    loss = self.loss(logits,labels.type_as(logits))\n",
        "    np.set_printoptions(precision=3,threshold=np.inf)\n",
        "    logit_list = logits.cpu().numpy()\n",
        "    label_list = labels.cpu().numpy()\n",
        "\n",
        "    print(logit_list)\n",
        "    np.savetxt('logits.txt',logit_list)\n",
        "    # with open('logits.txt','w') as w:\n",
        "    #   for i in logit_list:\n",
        "    #     out = np.array_str(i) \n",
        "    #     w.write(out)\n",
        "    np.savetxt('label.txt',label_list)  \n",
        "    # with open('label.txt','w') as wf:\n",
        "    #   for i in label_list:\n",
        "    #     out = np.array_str(i) \n",
        "    #     wf.write(out)\n",
        "    #     wf.write('\\n')\n",
        "    accuracy = self.get_multi_label_exact_match_accuracy(logits,labels)\n",
        "    self.logger.log_metrics({\n",
        "        'test_accuracy':accuracy.detach().cpu().numpy(),\n",
        "        'test_loss':loss.detach().cpu().numpy(),\n",
        "        'epoch': self.current_epoch,\n",
        "    })\n",
        "    return {'loss':loss,'test_accuracy':accuracy}\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer =  AdamW(self.parameters(), lr=LEARNING_RATE, eps=1e-12, betas=(0.9,0.999))\n",
        "    num_minibatch_steps = NUM_TRAIN_SAMPLES/BATCH_SIZE\n",
        "    max_epochs = MAX_EPOCHS\n",
        "    warmup = WARMUP\n",
        "    t_total = max_epochs * num_minibatch_steps\n",
        "    num_cycles = MAX_CYCLES\n",
        "    lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, warmup, t_total,num_cycles=num_cycles)\n",
        "    return [optimizer] ,[lr_scheduler]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYPizfX-9D_d"
      },
      "source": [
        "## multi-head transformer (used for doing multi prediction tasks at the same time, abandoned right now)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdG4yNr5L5DD"
      },
      "source": [
        "class ClassifierHead(nn.Module):\n",
        "  def __init__(self,hidden):\n",
        "    super().__init__()\n",
        "    self.class_value = nn.Linear(hidden,1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.loss = nn.BCELoss()\n",
        "\n",
        "  def get_label_and_loss(self,features,labels):\n",
        "    opx = self(features)\n",
        "    return opx, self.loss(opx,labels.float())\n",
        "\n",
        "  def forward(self,features):\n",
        "    return self.sigmoid(self.class_value(features))\n",
        "\n",
        "class SoftmaxClassifierHead(nn.Module):\n",
        "  def __init__(self,hidden):\n",
        "    super().__init__()\n",
        "    self.class_value = nn.Linear(hidden,2)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.sfmx = nn.Softmax(dim=1)\n",
        "    self.loss = nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "  def get_label_and_loss(self,features,labels):\n",
        "    opx = self(features)\n",
        "    # print(labels[0])\n",
        "    return torch.argmax(opx,dim=1).unsqueeze(1), self.loss(opx,labels.squeeze(1).long())\n",
        "\n",
        "  def forward(self,features):\n",
        "    return self.sfmx(self.class_value(features))\n",
        "\n",
        "class MultiHeadedBinaryClassifier(nn.Module):\n",
        "  def __init__(self,hidden,num_labels=NUM_LABELS):\n",
        "    super().__init__()\n",
        "    self.init_multi_head_label(hidden,num_labels)\n",
        "    self.num_labels = num_labels\n",
        "\n",
        "  @staticmethod\n",
        "  def get_head_name(idx):\n",
        "    return f'class_head_{idx}'\n",
        "\n",
        "  def get_class_head(self,idx):\n",
        "    head_name = self.get_head_name(idx)\n",
        "    return getattr(self,head_name)\n",
        "\n",
        "  def get_head_predictions(self,features,idx):\n",
        "    head = self.get_class_head(idx)\n",
        "    return head(features)\n",
        "\n",
        "  def init_multi_head_label(self,hidden,num_labels):\n",
        "    for i in range(num_labels):\n",
        "      setattr(self,self.get_head_name(i),SoftmaxClassifierHead(hidden))\n",
        "\n",
        "  def label_head_preds(self,features):\n",
        "    pred_ops = []\n",
        "    for i in range(self.num_labels):\n",
        "      pred_ops.append(self.get_head_predictions(features,i))\n",
        "    return torch.cat(pred_ops,dim=1)\n",
        "\n",
        "  def forward(self,features):\n",
        "    return self.label_head_preds(features)\n",
        "  \n",
        "  def get_labels_and_losses(self,features,labels):\n",
        "    label_tups = torch.split(labels,1,dim=1)\n",
        "    losses,pred_vals = [],[]\n",
        "    for idx,lab in enumerate(label_tups):\n",
        "      head = self.get_class_head(idx)\n",
        "      preds,loss = head.get_label_and_loss(features,lab)\n",
        "      losses.append(loss)\n",
        "      pred_vals.append(preds)\n",
        "    mean_loss = torch.sum(torch.stack(losses))\n",
        "    # print(pred_vals[0])\n",
        "    preds = torch.cat(pred_vals,dim=1)\n",
        "    return loss,preds\n",
        "\n",
        "\n",
        "class DataIntegrationColumnMultiHeadClassifier(pl.LightningModule):\n",
        "  def __init__(self,with_mask=False):\n",
        "    super().__init__()\n",
        "    # self.model =  TransformerDIClassifier()\n",
        "    self.with_mask = with_mask\n",
        "    \n",
        "    print(\"Using Vanilla Backbone Model\")\n",
        "    self.model = BertEmbedTransformer(**CORE_TRANSFORMER_PARAMS)\n",
        "      # self.model = TransformerDIClassifier()\n",
        "    # self.cross_entropy = nn.CrossEntropyLoss(torch.Tensor(WEIGHTS))\n",
        "    \n",
        "    self.multihead_class = MultiHeadedBinaryClassifier(CORE_TRANSFORMER_PARAMS['common_conv_dim'])\n",
        "    self.sfmx = nn.Softmax(dim=1)\n",
        "    self.sigmoid_threshold = SIGMOID_THRESHOLD\n",
        "    \n",
        "  def forward(self,inputs,att_mask=None): # state,action should be full trajectory sequences for state and action for each element in the batch.  \n",
        "    '''\n",
        "    inputs : (input_ids)\n",
        "        - input_ids: b k :\n",
        "          - b: batchsize\n",
        "          - k: sequence_length\n",
        "    *_mask = mask: b s : binary tensor. \n",
        "    '''\n",
        "    return self.model(inputs,att_mask=att_mask)\n",
        "\n",
        "  def get_multi_label_exact_match_accuracy(self,y_pred,y_true):\n",
        "    # print(y_pred[0])\n",
        "    # print(y_true[0])\n",
        "    pred_vals = y_pred.long()\n",
        "    num_vals = pred_vals.size()[0]\n",
        "    correct_vals = 0 # Exactly Correct Vals\n",
        "    for v1,v2 in zip(pred_vals,y_true):\n",
        "      if str(v1.cpu().numpy()) == str(v2.cpu().numpy()):\n",
        "        correct_vals+=1\n",
        "    return torch.tensor(correct_vals/num_vals)\n",
        "\n",
        "  \n",
        "\n",
        "  def training_step(self,batch,batch_nb):\n",
        "    inps,mask,labels = batch\n",
        "    # labels=(labels).type(torch.LongTensor).to(self.device)\n",
        "    if not self.with_mask:\n",
        "      features = self(inps,att_mask=None)\n",
        "    else:\n",
        "      features = self(inps,att_mask=mask)    \n",
        "    loss, logits = self.multihead_class.get_labels_and_losses(features,labels)\n",
        "    accuracy = self.get_multi_label_exact_match_accuracy(logits,labels)\n",
        "    self.logger.log_metrics({\n",
        "        'train_accuracy':accuracy.detach().cpu().numpy(),\n",
        "        # 'train_loss_embed':(loss_embed_pc+loss_embed_nc).detach().cpu().numpy(),\n",
        "        'train_loss':loss.detach().cpu().numpy(),\n",
        "        'epoch': self.current_epoch,\n",
        "    })\n",
        "    return {'loss':loss}\n",
        "\n",
        "\n",
        "  def validation_step(self,batch,batch_nb):\n",
        "    inps,mask,labels = batch\n",
        "    # labels=(labels).type(torch.LongTensor).to(self.device)\n",
        "    if not self.with_mask:\n",
        "      features = self(inps,att_mask=None)\n",
        "    else:\n",
        "      features = self(inps,att_mask=mask)    \n",
        "    loss, logits = self.multihead_class.get_labels_and_losses(features,labels)\n",
        "    \n",
        "    accuracy = self.get_multi_label_exact_match_accuracy(logits,labels)\n",
        "    self.logger.log_metrics({\n",
        "        'val_accuracy':accuracy.detach().cpu().numpy(),\n",
        "        'val_loss':loss.detach().cpu().numpy(),\n",
        "        'epoch': self.current_epoch,\n",
        "    })\n",
        "    return {'loss':loss,'val_loss':loss}\n",
        "  \n",
        "  def test_step(self,batch,batch_nb):\n",
        "    inps,mask,labels = batch\n",
        "    # labels=(labels).type(torch.LongTensor).to(self.device)\n",
        "    if not self.with_mask:\n",
        "      features = self(inps,att_mask=None)\n",
        "    else:\n",
        "      features = self(inps,att_mask=mask)    \n",
        "    loss, logits = self.multihead_class.get_labels_and_losses(features,labels)\n",
        "    \n",
        "    accuracy = self.get_multi_label_exact_match_accuracy(logits,labels)\n",
        "    self.logger.log_metrics({\n",
        "        'test_accuracy':accuracy.detach().cpu().numpy(),\n",
        "        'test_loss':loss.detach().cpu().numpy(),\n",
        "        'epoch': self.current_epoch,\n",
        "    })\n",
        "    return {'loss':loss,'test_accuracy':accuracy}\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer =  AdamW(self.parameters(), lr=LEARNING_RATE, eps=1e-12, betas=(0.9,0.999))\n",
        "    num_minibatch_steps = NUM_TRAIN_SAMPLES/BATCH_SIZE\n",
        "    max_epochs = MAX_EPOCHS\n",
        "    warmup = WARMUP\n",
        "    t_total = max_epochs * num_minibatch_steps\n",
        "    num_cycles = MAX_CYCLES\n",
        "    lr_scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, warmup, t_total,num_cycles=num_cycles)\n",
        "    return [optimizer] ,[lr_scheduler]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUyZXnk1_Tai"
      },
      "source": [
        "# Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QQ2tUeP_bA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3d7dd91-4135-445c-d016-f5637c2d3a0e"
      },
      "source": [
        "if EXP_TYPE == 'KEY':\n",
        "  model = DataIntegrationClassifier(with_mask=IS_MASK,is_bert=IS_BERT)\n",
        "elif EXP_TYPE == 'COL':\n",
        "  model = DataIntegrationColumnClassifier(with_mask=IS_MASK,is_bert=IS_BERT)\n",
        "  # model = DataIntegrationColumnMultiHeadClassifier(with_mask=IS_MASK)\n",
        "else:\n",
        "  raise Exception(\"Wrong EXP_TYPE\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Vanilla Backbone Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGwgTKNJ8qiU",
        "outputId": "d06a7041-16d3-4a21-b5e7-d33d435630f4"
      },
      "source": [
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import NeptuneLogger\n",
        "NOTE = '''\n",
        "Denser Backbone for Multi-label classification\n",
        "'''\n",
        "PROJECT_NAME = 'Data-Integration-Transformer'\n",
        "EXPERIMENT_NAME= f'Key-Implementation-{USE_CASE}-Multihead'\n",
        "pl.trainer.seed_everything(42)\n",
        "# CORE_TRANFORMER_PARAMS.update({'note':NOTE})\n",
        "logger_config_added_options = dict(\n",
        "    SAMPLE = SAMPLE,\n",
        "    BATCH_SIZE = BATCH_SIZE,\n",
        "    LEARNING_RATE = LEARNING_RATE,\n",
        "    WEIGHTS = WEIGHTS,\n",
        "    USE_CASE = USE_CASE,\n",
        "    NUM_LABELS = NUM_LABELS,\n",
        "    IS_BERT = IS_BERT,\n",
        "    IS_MASK=IS_MASK,\n",
        "    EXP_TYPE=EXP_TYPE,\n",
        "    CORE_TRANSFORMER_PARAMS=CORE_TRANSFORMER_PARAMS\n",
        ")\n",
        "\n",
        "# param_dict = {}\n",
        "API_TOKEN = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiOWM2Yzk2YzQtMzM4YS00ODEwLWE0ODgtNDQzOGEyYThmNTQ5In0=\"\n",
        "neptune_logger = NeptuneLogger(experiment_name=EXPERIMENT_NAME,\n",
        "                             project_name=f'zwang578/{PROJECT_NAME}',\n",
        "                             api_key = API_TOKEN,\n",
        "                             params={**logger_config_added_options},\n",
        "                             description=NOTE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n",
            "NeptuneLogger will work in online mode\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wlB1r-DVgmh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkaFu1RW8svY"
      },
      "source": [
        "NUM_TRAIN_SAMPLES = len(train_dataset)\n",
        "# Instantiate ModelCheckpoint\n",
        "model_checkpoint = ModelCheckpoint(filename='model/checkpoints/{epoch:02d}-{val_loss:.2f}',\n",
        "                                   save_weights_only=True,\n",
        "                                   save_top_k=3,\n",
        "                                   monitor='val_loss',\n",
        "                                   period=1)\n",
        "trainer = Trainer(\n",
        "    automatic_optimization=True,\n",
        "    max_epochs=MAX_EPOCHS,\\\n",
        "    progress_bar_refresh_rate=25,\\\n",
        "    gpus=1,\\\n",
        "    logger=neptune_logger,\n",
        "    checkpoint_callback=model_checkpoint\n",
        ")\n",
        "\n",
        "\n",
        "trainer.fit(model, train_loader,val_dataloaders=val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKWKErCOAf9i"
      },
      "source": [
        "trainer.test(model, test_dataloaders=DataLoader(test_dataset,batch_size=2048,shuffle=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSk5RRh4iUbk"
      },
      "source": [
        "def get_test_sest_acc(exp_id):\n",
        "  import neptune\n",
        "  project = neptune.init(f'zwang578/{PROJECT_NAME}',\n",
        "                          api_token=API_TOKEN\n",
        "                          )\n",
        "  my_exp = project.get_experiments(id=exp_id)\n",
        "  last_exp = my_exp[-1]\n",
        "  if EXP_TYPE == 'KEY':\n",
        "    testacc_values = last_exp.get_numeric_channels_values('test_accuracy','test_f1')\n",
        "  else:\n",
        "    testacc_values = last_exp.get_numeric_channels_values('test_accuracy')\n",
        "  return testacc_values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5vb5qO3xsKW"
      },
      "source": [
        "acc_df = get_test_sest_acc(neptune_logger.experiment.id)\n",
        "neptune_logger.experiment.set_property('test_set_accuracy_mean', acc_df['test_accuracy'].mean())\n",
        "neptune_logger.experiment.set_property('test_set_accuracy_variance', acc_df['test_accuracy'].var())\n",
        "if 'test_f1' in acc_df.columns:\n",
        "  neptune_logger.experiment.set_property('test_set_f1_mean', acc_df['test_f1'].mean())\n",
        "  neptune_logger.experiment.set_property('test_set_f1_var', acc_df['test_f1'].var())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRRJQfdjAklW"
      },
      "source": [
        "\n",
        "neptune_logger.experiment.id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKW83Fs8YISJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}